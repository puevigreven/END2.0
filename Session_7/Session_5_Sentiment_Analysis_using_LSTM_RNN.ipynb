{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Session 5 : Sentiment Analysis using LSTM RNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ripQ1EgDiYvk"
      },
      "source": [
        "! pip install nlpaug -q\n",
        "! pip install fairseq -q\n",
        "! pip install sacremoses -q\n",
        "! pip install fastBPE -q\n",
        "! pip install tqdm -q\n",
        "! pip install pyprind -q\n",
        "! pip install transformers -q \n"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3FaUFkt6tgz",
        "outputId": "c062516d-d020-4d90-854b-003050f584e1"
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyYCHIPExhgp"
      },
      "source": [
        "import nlpaug.augmenter.char as nac\n",
        "import nlpaug.augmenter.word as naw\n",
        "import nlpaug.augmenter.sentence as nas\n",
        "import nlpaug.flow as naf\n",
        "\n",
        "from nlpaug.util import Action\n",
        "\n",
        "import pandas as pd\n",
        "import tqdm\n",
        "import pickle\n",
        "import pyprind \n",
        "import pandas as pd"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qxpd-UmyzNf0"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WokR8QtBw35V"
      },
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/END2.0/SentimentClassification/datasetSentences.txt\", sep='\\t' )"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynkQUYMKqmM3"
      },
      "source": [
        "with open('/content/drive/MyDrive/END2.0/SentimentClassification/datasetSentences.txt') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "dataset = [line.split('\\t') for line in lines]  \n",
        "dataset = pd.DataFrame(dataset, columns = ['num', 'text']) \n",
        "dataset = dataset.drop([0]) # dropping first row which is 'sentence_index\" and \"sentence\" \n",
        "dataset = dataset.astype({'num': 'int64', 'text': 'string'})"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCBmyLT_x6-f"
      },
      "source": [
        "with open('/content/drive/MyDrive/END2.0/SentimentClassification/sentiment_labels.txt') as f:\n",
        "    lines = f.readlines()\n",
        "    \n",
        "dataset_label = [line.rstrip(\"\\n\").split(\"|\") for line in lines]  \n",
        "dataset_label = pd.DataFrame(dataset_label, columns = ['num', 'label'])\n",
        "dataset_label = dataset_label.drop([0]) # dropping first row which is 'phrase ids\" and \"sentiment values\" \n",
        "dataset_label = dataset_label.astype({'num': 'int64', 'label': 'float64'})"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJfbPeewyTpq",
        "outputId": "05fa8f59-214b-4925-8fc2-2b829671e530"
      },
      "source": [
        "with open('/content/drive/MyDrive/END2.0/SentimentClassification/datasetSplit.txt') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "dataset_split = [line.rstrip(\"\\n\").split(\",\") for line in lines]  \n",
        "dataset_split = pd.DataFrame(dataset_split, columns = ['num', 'split'])\n",
        "dataset_split = dataset_split.drop([0]) # dropping first row which is 'sentence_index' and \"splitset_label\" \n",
        "dataset_split = dataset_split.astype({'num': 'int64', 'split': 'int32'})\n",
        "dataset_split.dtypes"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "num      int64\n",
              "split    int32\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "mbaIBfkQyalk",
        "outputId": "1348be61-fe68-4910-fc77-ff8bbeeaf1e4"
      },
      "source": [
        "\n",
        "dataset = dataset.merge(dataset_label, on='num',how='inner')\n",
        "dataset = dataset.merge(dataset_split, on='num',how='inner')\n",
        "dataset.head()"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>num</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>split</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
              "      <td>0.50000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
              "      <td>0.44444</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Effective but too-tepid biopic</td>\n",
              "      <td>0.50000</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>If you sometimes like to go to the movies to h...</td>\n",
              "      <td>0.42708</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Emerges as something rare , an issue movie tha...</td>\n",
              "      <td>0.37500</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   num                                               text    label  split\n",
              "0    1  The Rock is destined to be the 21st Century 's...  0.50000      1\n",
              "1    2  The gorgeously elaborate continuation of `` Th...  0.44444      1\n",
              "2    3                    Effective but too-tepid biopic\n",
              "  0.50000      2\n",
              "3    4  If you sometimes like to go to the movies to h...  0.42708      2\n",
              "4    5  Emerges as something rare , an issue movie tha...  0.37500      2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgmOCuEdyfBv"
      },
      "source": [
        "def binify(row):\n",
        "  # print(row)\n",
        "  bin = 5\n",
        "  for i in range(bin):\n",
        "    if i/bin <= row < (i+1)/bin:\n",
        "      return i\n",
        "    elif row == 1.0:\n",
        "      return bin - 1"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 48
        },
        "id": "97K5eOcvyh5u",
        "outputId": "c9877e06-0f9f-45cd-d59f-6e867540e81f"
      },
      "source": [
        "dataset['label_distint'] = dataset['label'].apply(binify)\n",
        "\n",
        "dataset.label_distint.value_counts()\n",
        "\n",
        "dataset[dataset.label_distint.isna()]"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>num</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>split</th>\n",
              "      <th>label_distint</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [num, text, label, split, label_distint]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwCPFVB-5Ymj",
        "outputId": "4b199bd0-d0b2-43d7-9ba0-efbed6641e49"
      },
      "source": [
        "dataset.label_distint.value_counts()"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2    6240\n",
              "1    2175\n",
              "3    2139\n",
              "4     663\n",
              "0     638\n",
              "Name: label_distint, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "AEm1Z4G3ykSA",
        "outputId": "0596c90e-206a-4758-b2b4-a5c37d0e446b"
      },
      "source": [
        "dataset.head()"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>num</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>split</th>\n",
              "      <th>label_distint</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
              "      <td>0.50000</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
              "      <td>0.44444</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Effective but too-tepid biopic</td>\n",
              "      <td>0.50000</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>If you sometimes like to go to the movies to h...</td>\n",
              "      <td>0.42708</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Emerges as something rare , an issue movie tha...</td>\n",
              "      <td>0.37500</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   num                                               text  ...  split  label_distint\n",
              "0    1  The Rock is destined to be the 21st Century 's...  ...      1              2\n",
              "1    2  The gorgeously elaborate continuation of `` Th...  ...      1              2\n",
              "2    3                    Effective but too-tepid biopic\n",
              "  ...      2              2\n",
              "3    4  If you sometimes like to go to the movies to h...  ...      2              2\n",
              "4    5  Emerges as something rare , an issue movie tha...  ...      2              1\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_9VJKKDyplt"
      },
      "source": [
        "dataset.to_csv(\"/content/drive/MyDrive/END2.0/SentimentClassification/stanfordSentimentTreebank_dataset.csv\")"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ciss4XUejbnQ",
        "outputId": "ad9e6156-06d8-4bc0-b5de-aab794bda6b9"
      },
      "source": [
        "text = 'The quick brown fox jumps over the lazy dog .'\n",
        "print(text)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The quick brown fox jumps over the lazy dog .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FZWQrbWjWMa",
        "outputId": "63247540-a655-48b2-a15d-be6e90f5f319"
      },
      "source": [
        "\n",
        "aug = naw.SynonymAug(aug_src='wordnet')\n",
        "augmented_text = aug.augment(text)\n",
        "print(\"Original:\")\n",
        "print(text)\n",
        "print(\"Augmented Text:\")\n",
        "print(augmented_text)\n"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:\n",
            "The quick brown fox jumps over the lazy dog .\n",
            "Augmented Text:\n",
            "The quick brown fox spring over the indolent blackguard.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueX4n3EOrtaI",
        "outputId": "b05844c9-1849-4c4a-c4dd-437f8e8bb778"
      },
      "source": [
        "aug_del = naw.RandomWordAug()\n",
        "augmented_text = aug.augment(text)\n",
        "print(\"Original:\")\n",
        "print(text)\n",
        "print(\"Augmented Text:\")\n",
        "print(augmented_text)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:\n",
            "The quick brown fox jumps over the lazy dog .\n",
            "Augmented Text:\n",
            "The agile brown fox jump out over the lazy bounder.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwaYsyTOcpBJ"
      },
      "source": [
        "# import nlpaug.augmenter.word as naw\n",
        "\n",
        "# text = 'i am sleepig on the floor'\n",
        "# back_translation_aug = naw.BackTranslationAug(\n",
        "#     from_model_name='transformer.wmt19.en-de', \n",
        "#     to_model_name='transformer.wmt19.de-en'\n",
        "# )\n",
        "# augmented_text = back_translation_aug.augment(text)\n",
        "# print(\"Original:\")\n",
        "# print(text)\n",
        "# print(\"Augmented Text:\")\n",
        "# print(augmented_text)"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0PvJCfxrMST",
        "outputId": "f09423e4-b561-4584-bef1-513e40c9ed1d"
      },
      "source": [
        "aug = naf.Sometimes([\n",
        "    aug\n",
        "],aug_p=0.5, pipeline_p=0.5)\n",
        "\n",
        "aug.augment(text, n=4)\n"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i am sleepig on the story',\n",
              " 'i be sleepig on the floor',\n",
              " 'i am sleepig on the base',\n",
              " 'i am sleepig on the level']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BbZVXPrb-IO"
      },
      "source": [
        "dataset = pd.read_csv(\"/content/drive/MyDrive/END2.0/SentimentClassification/stanfordSentimentTreebank_dataset.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5Ii53hXcF-H",
        "outputId": "370a6370-b246-4ba1-feba-4727b24fd964"
      },
      "source": [
        "dataset.columns"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['num', 'text', 'label', 'split', 'label_distint'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKnwg7FlcJkN",
        "outputId": "dce05177-2294-4191-df80-590b84d1dfa3"
      },
      "source": [
        "(dataset.label_distint.value_counts())"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2    6240\n",
              "1    2175\n",
              "3    2139\n",
              "4     663\n",
              "0     638\n",
              "Name: label_distint, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V91ViW40dm9T"
      },
      "source": [
        "df_n=dataset[dataset.label_distint==1].reset_index(drop=True)"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9ruByEHbtcZ",
        "outputId": "b8a27859-fc48-4949-90f8-1aea054d2cf6"
      },
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def augment_text(df,samples=50,pr=0.2):\n",
        " \n",
        "    new_text=[]\n",
        "    for i in range(5):\n",
        "    ##selecting the minority class samples\n",
        "        df_n=df[df.label_distint==i].reset_index(drop=True)\n",
        "        ## data augmentation loop\n",
        "        print(np.random.randint(1,10,1))\n",
        "        for j in tqdm(np.random.randint(0,len(df_n),samples)):\n",
        "            \n",
        "                text = df_n.iloc[j]['text']\n",
        "                augmented_text = aug.augment(text)\n",
        "                new_text.append(augmented_text)\n",
        "        \n",
        "        \n",
        "        ## dataframe\n",
        "        new=pd.DataFrame({'text':new_text,'label_distint':i})\n",
        "        df=df.append(new).reset_index(drop=True)\n",
        "    return df\n",
        "   \n",
        "\n",
        "train_ = augment_text(dataset)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 50/50 [00:00<00:00, 290.51it/s]\n",
            "\n",
            "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[8]\n",
            "[7]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 50/50 [00:00<00:00, 289.75it/s]\n",
            "\n",
            "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 50/50 [00:00<00:00, 375.03it/s]\n",
            "\n",
            "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[3]\n",
            "[8]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 50/50 [00:00<00:00, 352.23it/s]\n",
            "\n",
            "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 50/50 [00:00<00:00, 346.11it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[8]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QERuRsOSnb9t"
      },
      "source": [
        "# train_.label_distint.value_counts()"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_FPo2cbU1xL",
        "outputId": "053b1596-a9b1-402a-888a-944d21a8c277"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Jun 24 16:49:38 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVjc4RxrUmjJ",
        "outputId": "34fcc02d-d6d8-4726-f788-6839e8494784"
      },
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your runtime has 13.6 gigabytes of available RAM\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYiRsFGD6iUC"
      },
      "source": [
        "# 0 TorchText"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tqdHKT3ARnl"
      },
      "source": [
        "import pandas as pd\n",
        "import pyprind\n",
        "import os, pickle"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tp5IzBGsPGHs"
      },
      "source": [
        "## Dataset Preview\n",
        "\n",
        "Your first step to deep learning in NLP. We will be mostly using PyTorch. Just like torchvision, PyTorch provides an official library, torchtext, for handling text-processing pipelines. \n",
        "\n",
        "We will be using previous session tweet dataset. Let's just preview the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKH1qJWBvDsU"
      },
      "source": [
        "## Preparing Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WFgVCc03sbZ"
      },
      "source": [
        "dataset = pd.read_csv(\"/content/drive/MyDrive/END2.0/SentimentClassification/stanfordSentimentTreebank_dataset.csv\")"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJ6o_79ISSVb"
      },
      "source": [
        "## Defining Fields"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e63g08ijOrf7"
      },
      "source": [
        "Now we shall be defining LABEL as a LabelField, which is a subclass of Field that sets sequen tial to False (as it’s our numerical category class). TWEET is a standard Field object, where we have decided to use the spaCy tokenizer and convert all the text to lower‐ case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qk8IP4SK1Lrp",
        "outputId": "a77a8e23-2e01-4ba5-c2a5-b3daf3256d99"
      },
      "source": [
        "# Import Library\n",
        "import random\n",
        "import torch, torchtext\n",
        "from torchtext.legacy import data\n",
        "import spacy\n",
        "sp = spacy.load('en_core_web_sm')\n",
        "import random\n",
        "\n",
        "# Manual Seed\n",
        "SEED = 43\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "all_stopwords = sp.Defaults.stop_words\n",
        "print(len(all_stopwords))"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "326\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xd8MUyzfTE_J"
      },
      "source": [
        "train_df = dataset[dataset['split'] == 1].reset_index()\n",
        "test_df = dataset[dataset['split'] == 2].reset_index()\n",
        "val_df = dataset[dataset['split'] == 3].reset_index()\n"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-I2t5X_j11-E"
      },
      "source": [
        "dataset_df = train_df.append(test_df).append(val_df).reset_index()"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "eVI4ia_B2DTJ",
        "outputId": "7267296f-eb2f-4de8-80d3-1f1430593814"
      },
      "source": [
        "dataset_df.head()"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>level_0</th>\n",
              "      <th>index</th>\n",
              "      <th>num</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>split</th>\n",
              "      <th>label_distint</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
              "      <td>0.50000</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
              "      <td>0.44444</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>60</td>\n",
              "      <td>61</td>\n",
              "      <td>Singer\\/composer Bryan Adams contributes a sle...</td>\n",
              "      <td>0.47222</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>61</td>\n",
              "      <td>62</td>\n",
              "      <td>You 'd think by now America would have had eno...</td>\n",
              "      <td>0.33333</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>62</td>\n",
              "      <td>63</td>\n",
              "      <td>Yet the act is still charming here .</td>\n",
              "      <td>0.31944</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   level_0  index  num  ...    label  split  label_distint\n",
              "0        0      0    1  ...  0.50000      1              2\n",
              "1        1      1    2  ...  0.44444      1              2\n",
              "2        2     60   61  ...  0.47222      1              2\n",
              "3        3     61   62  ...  0.33333      1              1\n",
              "4        4     62   63  ...  0.31944      1              1\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX-lYIe_O7Vy"
      },
      "source": [
        "Having defined those fields, we now need to produce a list that maps them onto the list of rows that are in the CSV:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCg7WvM32T2r"
      },
      "source": [
        "Text = data.Field(sequential = True, tokenize = 'spacy', batch_first =True, include_lengths=True, stop_words = all_stopwords)\n",
        "Label = data.LabelField(tokenize ='spacy', is_target=True, batch_first =True, sequential =False)"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VawdWq36O6td"
      },
      "source": [
        "fields = [('text', Text),('labels',Label)]"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbtZ-Ph2P1xL"
      },
      "source": [
        "Armed with our declared fields, lets convert from pandas to list to torchtext. We could also use TabularDataset to apply that definition to the CSV directly but showing an alternative approach too."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOXjVEOxIiDg"
      },
      "source": [
        "# train_example = pickle.load( \n",
        "#     open( \"/content/drive/MyDrive/END2.0/SentimentClassification/train_examples.pkl\", \"rb\" ) )\n",
        "# val_example = pickle.load( \n",
        "#     open( \"/content/drive/MyDrive/END2.0/SentimentClassification/val_examples.pkl\", \"rb\" ) )\n",
        "# test_example = pickle.load( \n",
        "#     open( \"/content/drive/MyDrive/END2.0/SentimentClassification/test_examples.pkl\", \"rb\" ) )\n"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hErcJfNT7Gj"
      },
      "source": [
        "train_example = [data.Example.fromlist([dataset_df.text[i],dataset_df.label_distint[i]], fields) for i in range(dataset_df.shape[0])] \n",
        "# test_example = [data.Example.fromlist([test_df.text[i],test_df.label_distint[i]], fields) for i in range(test_df.shape[0])] \n",
        "# val_example = [data.Example.fromlist([val_df.text[i],val_df.label_distint[i]], fields) for i in range(val_df.shape[0])] "
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nT-flpH-P1cd"
      },
      "source": [
        "dataset = data.Dataset(train_example, fields)\n",
        "# test = data.Dataset(test_example, fields)\n",
        "# valid = data.Dataset(val_example, fields)"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIoEESgc2ddJ"
      },
      "source": [
        "(train, valid) = dataset.split(split_ratio=[70, 30], random_state = random.seed(SEED))"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykvsCGQMR6UD",
        "outputId": "a239e545-532a-46f9-8bf6-1e24a4f0c031"
      },
      "source": [
        "len(train), len(valid)"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8298, 3557)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kix8P2IKSBaV"
      },
      "source": [
        "An example from the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUpEOQruR9JL",
        "outputId": "41c0762a-e8a5-4d9c-ad28-4198f70df09e"
      },
      "source": [
        "vars(train.examples[1])"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'labels': 2,\n",
              " 'text': ['Instead',\n",
              "  'building',\n",
              "  'laugh',\n",
              "  'riot',\n",
              "  'left',\n",
              "  'handful',\n",
              "  'disparate',\n",
              "  'funny',\n",
              "  'moments',\n",
              "  'real',\n",
              "  'consequence',\n",
              "  '.']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKdllP3FST4N"
      },
      "source": [
        "## Building Vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuvWQ-SpSmSz"
      },
      "source": [
        "At this point we would have built a one-hot encoding of each word that is present in the dataset—a rather tedious process. Thankfully, torchtext will do this for us, and will also allow a max_size parameter to be passed in to limit the vocabu‐ lary to the most common words. This is normally done to prevent the construction of a huge, memory-hungry model. We don’t want our GPUs too overwhelmed, after all. \n",
        "\n",
        "Let’s limit the vocabulary to a maximum of 5000 words in our training set:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mx955u93SGeY"
      },
      "source": [
        "MAX_VOCAB_SIZE = 17_000\n",
        "\n",
        "Text.build_vocab(train, \n",
        "                 max_size = MAX_VOCAB_SIZE, \n",
        "                 vectors = \"glove.840B.300d\")\n",
        "Label.build_vocab(train)"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCvJc2UqsR3q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "481b052e-64ee-43f4-d605-5ffcc0a3f8a0"
      },
      "source": [
        "Text.vocab.vectors.shape"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16682, 300])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvyEeEjXTGhX"
      },
      "source": [
        "By default, torchtext will add two more special tokens, <unk> for unknown words and <pad>, a padding token that will be used to pad all our text to roughly the same size to help with efficient batching on the GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rA3tIESdcJdN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40be3852-7cd9-40b4-9f1a-349800148d9f"
      },
      "source": [
        "print('Size of input vocab : ', len(Text.vocab))\n",
        "print('Size of label vocab : ', len(Label.vocab))\n",
        "print('Top 10 words appreared repeatedly :', list(Text.vocab.freqs.most_common(10)))\n",
        "print('Labels : ', Label.vocab.stoi)"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of input vocab :  16682\n",
            "Size of label vocab :  5\n",
            "Top 10 words appreared repeatedly : [('.', 7806), (',', 7067), ('-', 2694), ('The', 1218), ('film', 1124), ('movie', 962), ('A', 825), ('`', 695), ('It', 655), ('...', 610)]\n",
            "Labels :  defaultdict(None, {2: 0, 1: 1, 3: 2, 0: 3, 4: 4})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04s7XvPI0xeQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "670a0faf-fd0b-413a-8d15-51cdb451d3a3"
      },
      "source": [
        "print(Text.vocab.itos[:10])"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<unk>', '<pad>', '.', ',', '-', 'The', 'film', 'movie', 'A', '`']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwjD2-ebTeUX"
      },
      "source": [
        "**Lots of stopwords!!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLWW221gTpNs"
      },
      "source": [
        "Now we need to create a data loader to feed into our training loop. Torchtext provides the BucketIterator method that will produce what it calls a Batch, which is almost, but not quite, like the data loader we used on images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQqMhMoDUDmn"
      },
      "source": [
        "But at first declare the device we are using."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zfo2QhGJUK4l"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zK2ORoqdTNsM"
      },
      "source": [
        "train_iterator, valid_iterator = data.BucketIterator.splits((train, valid), batch_size = 64, \n",
        "                                                            sort_key = lambda x: len(x.text),\n",
        "                                                            sort_within_batch=True, device = device)"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sseGRHmf1G6y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c28e121-1384-43ca-82d6-6bb1040b090b"
      },
      "source": [
        "print('Train')\n",
        "for batch in train_iterator:\n",
        "    print(f'Text matrix size: {batch.text[0].size()}')\n",
        "    print(f'Target vector size: {batch.labels.size()}')\n",
        "    break\n",
        "    \n",
        "print('\\nValid:')\n",
        "for batch in valid_iterator:\n",
        "    print(f'Text matrix size: {batch.text[0].size()}')\n",
        "    print(f'Target vector size: {batch.labels.size()}')\n",
        "    break\n",
        "    \n"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train\n",
            "Text matrix size: torch.Size([64, 6])\n",
            "Target vector size: torch.Size([64])\n",
            "\n",
            "Valid:\n",
            "Text matrix size: torch.Size([64, 3])\n",
            "Target vector size: torch.Size([64])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gg7gTFQO4fby"
      },
      "source": [
        "Save the vocabulary for later use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niE9Cc6-2bD_"
      },
      "source": [
        "\n",
        "with open('/content/drive/MyDrive/END2.0/SentimentClassification/tokenizer.pkl', 'wb') as tokens: \n",
        "  pickle.dump(Text.vocab.stoi, tokens)\n",
        "\n",
        "# with open('/content/drive/MyDrive/END2.0/SentimentClassification/train_examples.pkl', 'wb') as tokens: \n",
        "#     pickle.dump(train_example, tokens)\n",
        "\n",
        "# with open('/content/drive/MyDrive/END2.0/SentimentClassification/val_examples.pkl', 'wb') as tokens: \n",
        "#     pickle.dump(val_example, tokens)\n",
        "\n",
        "# with open('/content/drive/MyDrive/END2.0/SentimentClassification/test_examples.pkl', 'wb') as tokens: \n",
        "#     pickle.dump(test_example, tokens)\n",
        "\n",
        "\n"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AbsQwqkVyAy"
      },
      "source": [
        "## Defining Our Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4PED4HJWH4t"
      },
      "source": [
        "We use the Embedding and LSTM modules in PyTorch to build a simple model for classifying tweets.\n",
        "\n",
        "In this model we create three layers. \n",
        "1. First, the words in our tweets are pushed into an Embedding layer, which we have established as a 300-dimensional vector embedding. \n",
        "2. That’s then fed into a 2 stacked-LSTMs with 100 hidden features (again, we’re compressing down from the 300-dimensional input like we did with images). We are using 2 LSTMs for using the dropout.\n",
        "3. Finally, the output of the LSTM (the final hidden state after processing the incoming tweet) is pushed through a standard fully connected layer with three outputs to correspond to our three possible classes (negative, positive, or neutral)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43pVRccMT0bT"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class classifier(nn.Module):\n",
        "    \n",
        "    # Define all the layers used in model\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout):\n",
        "        \n",
        "        super().__init__()          \n",
        "        \n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        \n",
        "        # LSTM layer\n",
        "        self.encoder = nn.LSTM(embedding_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           dropout=dropout,\n",
        "                           batch_first=True,\n",
        "                           bidirectional= True)\n",
        "        # try using nn.GRU or nn.RNN here and compare their performances\n",
        "        # try bidirectional and compare their performances\n",
        "        \n",
        "        # Dense layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "    def forward(self, text, text_lengths):\n",
        "        \n",
        "        # text = [batch size, sent_length]\n",
        "        embedded = self.embedding(text)\n",
        "        # embedded = [batch size, sent_len, emb dim]\n",
        "      \n",
        "        # packed sequence\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu(), batch_first=True)\n",
        "        \n",
        "        # packed_output ,hidden = self.encoder(packed_embedded)\n",
        "        \n",
        "        packed_output, (hidden, cell) = self.encoder(packed_embedded)\n",
        "        #hidden = [batch size, num layers * num directions,hid dim]\n",
        "        #cell = [batch size, num layers * num directions,hid dim]\n",
        "        # Hidden = [batch size, hid dim * num directions]\n",
        "        dense_outputs = self.fc(hidden)   \n",
        "        \n",
        "        # Final activation function softmax\n",
        "        output = F.softmax(dense_outputs[0], dim=1)\n",
        "        # print (output)\n",
        "        return output"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwBoGE_X_Fl8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58fad539-f67d-4394-947d-61c68e73d75e"
      },
      "source": [
        "# Define hyperparameters\n",
        "size_of_vocab = len(Text.vocab)\n",
        "embedding_dim = 300\n",
        "num_hidden_nodes = 16\n",
        "num_output_nodes = 5\n",
        "num_layers = 1\n",
        "dropout = 0.5\n",
        "PAD_IDX = Text.vocab.stoi[Text.pad_token]\n",
        "# Instantiate the model\n",
        "model = classifier(size_of_vocab, embedding_dim, num_hidden_nodes, num_output_nodes, num_layers, dropout = dropout)"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPbLamPYsjC5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbdd3fb5-d6cc-405b-d4a9-baac0468c9a6"
      },
      "source": [
        "pretrained_embeddings = Text.vocab.vectors\n",
        "\n",
        "print(pretrained_embeddings.shape)"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([16682, 300])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Z1V6koJswUV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7be179b-9da7-41fe-b4d2-262d081c28ac"
      },
      "source": [
        "# model.embedding.weight.data = pretrained_embeddings.cuda()\n",
        "model.embedding.weight.data.copy_(pretrained_embeddings)\n"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0120,  0.2075, -0.1258,  ...,  0.1387, -0.3605, -0.0350],\n",
              "        ...,\n",
              "        [ 0.0164, -0.1268,  0.1124,  ..., -0.0723,  0.4662, -0.3872],\n",
              "        [-0.0592,  0.1091, -0.2313,  ...,  0.0914,  0.6806, -0.4423],\n",
              "        [-0.3185, -0.0888,  0.0675,  ..., -0.2871,  0.6534, -0.5551]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4Cu1uO5mYe7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5479ba8d-48b1-41aa-d469-22982d75370b"
      },
      "source": [
        "\n",
        "model.embedding.weight.data.shape"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16682, 300])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2MpDvknl7Xl"
      },
      "source": [
        "UNK_IDX = Text.vocab.stoi[Text.unk_token]\n",
        "\n",
        "model.embedding.weight.data[UNK_IDX] = torch.zeros(embedding_dim)\n",
        "model.embedding.weight.data[PAD_IDX] = torch.zeros(embedding_dim)"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-pOMqzJ3eTv",
        "outputId": "dc8b8f85-936f-4cd0-926e-3dade4cfbf10"
      },
      "source": [
        "print(model)\n",
        "\n",
        "#No. of trianable parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    \n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "classifier(\n",
            "  (embedding): Embedding(16682, 300)\n",
            "  (encoder): LSTM(300, 16, batch_first=True, dropout=0.5, bidirectional=True)\n",
            "  (fc): Linear(in_features=16, out_features=5, bias=True)\n",
            ")\n",
            "The model has 5,045,389 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXajorf5Xz7t"
      },
      "source": [
        "## Model Training and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrE9RpMtZ1Vs"
      },
      "source": [
        "First define the optimizer and loss functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-u86JWdlXvu5"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# define optimizer and loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# define metric\n",
        "def binary_accuracy(preds, y):\n",
        "    #round predictions to the closest integer\n",
        "    _, predictions = torch.max(preds, 1)\n",
        "    \n",
        "    correct = (predictions == y).float() \n",
        "    # print (predictions, y)\n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n",
        "\n",
        "def categorical_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "    top_pred = preds.argmax(1, keepdim = True)\n",
        "    # y_pred_tags = preds.argmax(dim=1) \n",
        "    # print(preds)\n",
        "    # print(top_pred, y)\n",
        "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
        "    acc = correct.float() / y.shape[0]\n",
        "    return acc    \n",
        "# push to cuda if available\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VCJtNb3Zt8w"
      },
      "source": [
        "The main thing to be aware of in this new training loop is that we have to reference `batch.tweets` and `batch.labels` to get the particular fields we’re interested in; they don’t fall out quite as nicely from the enumerator as they do in torchvision."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WjEPLKsAiS_"
      },
      "source": [
        "**Training Loop**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDWNnGK3Y5oJ"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    # initialize every epoch \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    epoch_f1 = 0\n",
        "    # set the model in training phase\n",
        "    model.train()  \n",
        "    bar = pyprind.ProgBar(len(iterator), bar_char='█')\n",
        "    for batch in iterator:\n",
        "        \n",
        "        # resets the gradients after every batch\n",
        "        optimizer.zero_grad()   \n",
        "        \n",
        "        # retrieve text and no. of words\n",
        "        text, text_lengths = batch.text   \n",
        "        \n",
        "        # convert to 1D tensor\n",
        "        predictions = model(text, text_lengths).squeeze()  \n",
        "        # print(list(predictions))\n",
        "        # compute the loss\n",
        "        loss = criterion(predictions, batch.labels)        \n",
        "        \n",
        "        # compute the category accuracy\n",
        "        acc = categorical_accuracy(predictions, batch.labels)   \n",
        "        true_y = F.one_hot(batch.labels, 5).to('cpu')\n",
        " \n",
        "        f1_scr = f1_score(true_y, predictions.data.to('cpu') > 0.5, average=\"samples\")\n",
        "        # backpropage the loss and compute the gradients\n",
        "        # print(epoch_f1)\n",
        "        loss.backward()       \n",
        "        \n",
        "        # update the weights\n",
        "        optimizer.step()      \n",
        "        \n",
        "        # loss and accuracy\n",
        "        epoch_loss += loss.item()  \n",
        "        epoch_acc += acc.item()  \n",
        "        epoch_f1 += f1_scr \n",
        "        bar.update()\n",
        "        # break\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_f1/ len(iterator)"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZcHhkkvAsCt"
      },
      "source": [
        "**Evaluation Loop**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHEe-zSVAriL"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    # initialize every epoch\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    epoch_f1 = 0\n",
        "\n",
        "    # deactivating dropout layers\n",
        "    model.eval()\n",
        "    \n",
        "    # deactivates autograd\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "        \n",
        "            # retrieve text and no. of words\n",
        "            text, text_lengths = batch.text\n",
        "            \n",
        "            # convert to 1d tensor\n",
        "            predictions = model(text, text_lengths).squeeze()\n",
        "            \n",
        "            # compute loss and accuracy\n",
        "            loss = criterion(predictions, batch.labels)\n",
        "            acc = categorical_accuracy(predictions, batch.labels)\n",
        "            true_y = F.one_hot(batch.labels, 5).to('cpu')\n",
        " \n",
        "            f1_scr = f1_score(true_y, predictions.data.to('cpu') > 0.5, average=\"samples\")\n",
        "            # keep track of loss and accuracy\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "            epoch_f1 += f1_scr\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_f1/ len(iterator)"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6LJFW7HaJoV"
      },
      "source": [
        "**Let's Train and Evaluate**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tq330XlnaEU9",
        "outputId": "6aad5fdb-b218-47d2-b53e-65828261e0ee"
      },
      "source": [
        "from sklearn.metrics import classification_report, f1_score\n",
        "N_EPOCHS = 5\n",
        "train_acc_list = []\n",
        "val_acc_list = []\n",
        "train_loss_list = []\n",
        "val_loss_list = []\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "     \n",
        "    # train the model\n",
        "    train_loss, train_acc, train_f1 = train(model, train_iterator, optimizer, criterion)\n",
        "    train_acc_list.append(train_acc)\n",
        "    train_loss_list.append(train_loss)\n",
        "    # evaluate the model\n",
        "    valid_loss, valid_acc, val_f1 = evaluate(model, valid_iterator, criterion)\n",
        "    val_loss_list.append(valid_loss)\n",
        "    val_acc_list.append(valid_acc)\n",
        "    # save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    print(f'\\tEPOCH: {epoch}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% |  F1 Score: {train_f1}')\n",
        "\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}% |  F1 Score: {val_f1}')\n",
        "    # break"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
            "Total time elapsed: 00:00:01\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tEPOCH: 0\n",
            "\tTrain Loss: 1.571 | Train Acc: 40.34% |  F1 Score: 0.015865384615384615\n",
            "\t Val. Loss: 1.494 |  Val. Acc: 53.26% |  F1 Score: 0.3714557191119691\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
            "Total time elapsed: 00:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tEPOCH: 1\n",
            "\tTrain Loss: 1.446 | Train Acc: 52.16% |  F1 Score: 0.4883356227106227\n",
            "\t Val. Loss: 1.409 |  Val. Acc: 53.31% |  F1 Score: 0.5261371862934363\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
            "Total time elapsed: 00:00:01\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tEPOCH: 2\n",
            "\tTrain Loss: 1.406 | Train Acc: 52.28% |  F1 Score: 0.5197744963369964\n",
            "\t Val. Loss: 1.390 |  Val. Acc: 53.31% |  F1 Score: 0.5328336148648648\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
            "Total time elapsed: 00:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tEPOCH: 3\n",
            "\tTrain Loss: 1.395 | Train Acc: 52.30% |  F1 Score: 0.5222928113553114\n",
            "\t Val. Loss: 1.384 |  Val. Acc: 53.31% |  F1 Score: 0.5328336148648648\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
            "Total time elapsed: 00:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tEPOCH: 4\n",
            "\tTrain Loss: 1.390 | Train Acc: 52.33% |  F1 Score: 0.5229681776556777\n",
            "\t Val. Loss: 1.380 |  Val. Acc: 53.31% |  F1 Score: 0.5331126327220077\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "29gt-1Vxn8ij",
        "outputId": "c9b7b9fb-5a5e-4f74-f490-808fa9108e54"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "loss_train = train_acc_list\n",
        "loss_val = val_acc_list\n",
        "epochs = range(0,N_EPOCHS)\n",
        "plt.plot(epochs, loss_train, 'g', label='Training accuracy')\n",
        "plt.plot(epochs, loss_val, 'b', label='validation accuracy')\n",
        "plt.title('Training and Validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZgzB0ZkHVTI"
      },
      "source": [
        "## Model Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QU1OckfuXFh",
        "outputId": "1cdab016-f34f-4cda-8054-1d6be05658f0"
      },
      "source": [
        "path='./saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path))\n",
        "model.eval()\n",
        "test_loss, test_acc,test_f1 = evaluate(model, valid_iterator, criterion)\n",
        "print(f'\\t Test. Loss: {test_loss:.3f} |  Test. Acc: {test_acc*100:.2f}% |  Test. F1: {test_f1}\\n')"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t Test. Loss: 1.380 |  Test. Acc: 53.31% |  Test. F1: 0.5331126327220077\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZZfnWo0abRx"
      },
      "source": [
        "#load weights and tokenizer\n",
        "\n",
        "path='./saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path));\n",
        "model.eval();\n",
        "tokenizer_file = open('/content/drive/MyDrive/END2.0/SentimentClassification/tokenizer.pkl', 'rb')\n",
        "tokenizer = pickle.load(tokenizer_file)\n",
        "\n",
        "#inference \n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "def classify_tweet(tweet):\n",
        "    # {2: 0, 1: 1, 3: 2, 4: 3, 0: 4}  very negative, negative, neutral, positive, very positive\n",
        "    categories = {0:\"neutral\", 1:\"negative\",2: \"positive\", 3:\"very positive\", 4:\"very negative\" }\n",
        "    \n",
        "    # tokenize the tweet \n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(tweet)] \n",
        "    # convert to integer sequence using predefined tokenizer dictionary\n",
        "    indexed = [tokenizer[t] for t in tokenized]        \n",
        "    # compute no. of words        \n",
        "    length = [len(indexed)]\n",
        "    # convert to tensor                                    \n",
        "    tensor = torch.LongTensor(indexed).to(device)   \n",
        "    # reshape in form of batch, no. of words           \n",
        "    tensor = tensor.unsqueeze(1).T  \n",
        "    # convert to tensor                          \n",
        "    length_tensor = torch.LongTensor(length)\n",
        "    # Get the model prediction                  \n",
        "    prediction = model(tensor, length_tensor)\n",
        "\n",
        "    _, pred = torch.max(prediction, 1) \n",
        "    \n",
        "    return categories[pred.item()]"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTkHLEipIlM9",
        "outputId": "be73e75d-f7c0-43cc-aaff-25ed57b9f7b6"
      },
      "source": [
        "dataset = pd.read_csv(\"/content/drive/MyDrive/END2.0/SentimentClassification/stanfordSentimentTreebank_dataset.csv\")\n",
        "test_df = dataset[dataset['split'] == 2].reset_index()\n",
        "import numpy as np\n",
        "categories = {0:\"very negative\", 1:\"negative\",2: \"neutral\", 3:\"positive\", 4:\"very positive\" }\n",
        "trail = 9\n",
        "list_rand = np.random.randint(1, test_df.shape[0], 10)\n",
        "print(list_rand)\n",
        "for index, i in test_df.iloc[list_rand].iterrows():\n",
        "    # print(i)\n",
        "    print(\"Sentence: \", i.text)\n",
        "    print(\"True Label: \", categories[i.label_distint])\n",
        "    print(\"Predicted: \" ,classify_tweet(i.text))\n",
        "    print(\"-\"*25)"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1074 1145 2021 1915  274  173  671 1888 1215  891]\n",
            "Sentence:  The film seems a dead weight .\n",
            "\n",
            "True Label:  very negative\n",
            "Predicted:  neutral\n",
            "-------------------------\n",
            "Sentence:  To imagine the life of Harry Potter as a martial arts adventure told by a lobotomized Woody Allen is to have some idea of the fate that lies in store for moviegoers lured to the mediocrity that is Kung Pow : Enter the Fist .\n",
            "\n",
            "True Label:  neutral\n",
            "Predicted:  neutral\n",
            "-------------------------\n",
            "Sentence:  It 's all very cute , though not terribly funny if you 're more than six years old .\n",
            "\n",
            "True Label:  neutral\n",
            "Predicted:  neutral\n",
            "-------------------------\n",
            "Sentence:  The story is less vibrant , the jokes are a little lukewarm , but will anyone really care ?\n",
            "\n",
            "True Label:  neutral\n",
            "Predicted:  neutral\n",
            "-------------------------\n",
            "Sentence:  Parker holds true to Wilde 's own vision of a pure comedy with absolutely no meaning , and no desire to be anything but a polished , sophisticated entertainment that is in love with its own cleverness .\n",
            "\n",
            "True Label:  very positive\n",
            "Predicted:  neutral\n",
            "-------------------------\n",
            "Sentence:  Ranks among Willams ' best screen work .\n",
            "\n",
            "True Label:  neutral\n",
            "Predicted:  neutral\n",
            "-------------------------\n",
            "Sentence:  Goyer 's screenplay and direction are thankfully understated , and he has drawn excellent performances from his cast .\n",
            "\n",
            "True Label:  very positive\n",
            "Predicted:  neutral\n",
            "-------------------------\n",
            "Sentence:  ... surprisingly inert for a movie in which the main character travels back and forth between epochs .\n",
            "\n",
            "True Label:  negative\n",
            "Predicted:  neutral\n",
            "-------------------------\n",
            "Sentence:  A simple , sometimes maddeningly slow film that has just enough charm and good acting to make it interesting , but is ultimately pulled under by the pacing and lack of creativity within .\n",
            "\n",
            "True Label:  positive\n",
            "Predicted:  neutral\n",
            "-------------------------\n",
            "Sentence:  This horror-comedy does n't go for the usual obvious laughs at the expense of cheap-looking monsters -- unless you count Elvira 's hooters .\n",
            "\n",
            "True Label:  neutral\n",
            "Predicted:  neutral\n",
            "-------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVjCuKK_LVEF"
      },
      "source": [
        "## Discussion on Data Augmentation Techniques \n",
        "\n",
        "You might wonder exactly how you can augment text data. After all, you can’t really flip it horizontally as you can an image! :D \n",
        "\n",
        "In contrast to data augmentation in images, augmentation techniques on data is very specific to final product you are building. As its general usage on any type of textual data doesn't provides a significant performance boost, that's why unlike torchvision, torchtext doesn’t offer a augmentation pipeline. Due to powerful models as transformers, augmentation tecnhiques are not so preferred now-a-days. But its better to know about some techniques with text that will provide your model with a little more information for training. \n",
        "\n",
        "### Synonym Replacement\n",
        "\n",
        "First, you could replace words in the sentence with synonyms, like so:\n",
        "\n",
        "    The dog slept on the mat\n",
        "\n",
        "could become\n",
        "\n",
        "    The dog slept on the rug\n",
        "\n",
        "Aside from the dog's insistence that a rug is much softer than a mat, the meaning of the sentence hasn’t changed. But mat and rug will be mapped to different indices in the vocabulary, so the model will learn that the two sentences map to the same label, and hopefully that there’s a connection between those two words, as everything else in the sentences is the same."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_uEfWJpL6Nq"
      },
      "source": [
        "### Random Insertion\n",
        "A random insertion technique looks at a sentence and then randomly inserts synonyms of existing non-stopwords into the sentence n times. Assuming you have a way of getting a synonym of a word and a way of eliminating stopwords (common words such as and, it, the, etc.), shown, but not implemented, in this function via get_synonyms() and get_stopwords(), an implementation of this would be as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Alm5D7WIvAC"
      },
      "source": [
        "def random_insertion(sentence, n): \n",
        "    words = remove_stopwords(sentence) \n",
        "    for _ in range(n):\n",
        "        new_synonym = get_synonyms(random.choice(words))\n",
        "        sentence.insert(randrange(len(sentence)+1), new_synonym) \n",
        "    return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqLWzwJ3Mm8h"
      },
      "source": [
        "## Random Deletion\n",
        "As the name suggests, random deletion deletes words from a sentence. Given a probability parameter p, it will go through the sentence and decide whether to delete a word or not based on that random probability. Consider of it as pixel dropouts while treating images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7Dz7JJfMqyC"
      },
      "source": [
        "def random_deletion(words, p=0.5): \n",
        "    if len(words) == 1: # return if single word\n",
        "        return words\n",
        "    remaining = list(filter(lambda x: random.uniform(0,1) > p,words)) \n",
        "    if len(remaining) == 0: # if not left, sample a random word\n",
        "        return [random.choice(words)] \n",
        "    else:\n",
        "        return remaining"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOIbi5WzO5OU"
      },
      "source": [
        "### Random Swap\n",
        "The random swap augmentation takes a sentence and then swaps words within it n times, with each iteration working on the previously swapped sentence. Here we sample two random numbers based on the length of the sentence, and then just keep swapping until we hit n."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnkbG15HO3Yj"
      },
      "source": [
        "def random_swap(sentence, n=5): \n",
        "    length = range(len(sentence)) \n",
        "    for _ in range(n):\n",
        "        idx1, idx2 = random.sample(length, 2)\n",
        "        sentence[idx1], sentence[idx2] = sentence[idx2], sentence[idx1] \n",
        "    return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "599NpwfMR5Vm"
      },
      "source": [
        "For more on this please go through this [paper](https://arxiv.org/pdf/1901.11196.pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5aeKuNCRGip"
      },
      "source": [
        "### Back Translation\n",
        "\n",
        "Another popular approach for augmenting text datasets is back translation. This involves translating a sentence from our target language into one or more other languages and then translating all of them back to the original language. We can use the Python library googletrans for this purpose. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHhNBbYrRXNy"
      },
      "source": [
        "import random\n",
        "import googletrans\n",
        "import googletrans.Translator\n",
        "\n",
        "translator = Translator()\n",
        "sentence = ['']\n",
        "\n",
        "available_langs = list(googletrans.LANGUAGES.keys()) \n",
        "trans_lang = random.choice(available_langs) \n",
        "print(f\"Translating to {googletrans.LANGUAGES[trans_lang]}\")\n",
        "\n",
        "translations = translator.translate(sentence, dest=trans_lang) \n",
        "t_text = [t.text for t in translations]\n",
        "print(t_text)\n",
        "\n",
        "translations_en_random = translator.translate(t_text, src=trans_lang, dest='en') \n",
        "en_text = [t.text for t in translations_en_random]\n",
        "print(en_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiscgzPjAQRI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2L6Uk-60dk60"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}