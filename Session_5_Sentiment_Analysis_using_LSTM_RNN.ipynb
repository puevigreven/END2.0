{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Session 5 : Sentiment Analysis using LSTM RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/puevigreven/END2.0/blob/main/Session_5_Sentiment_Analysis_using_LSTM_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ripQ1EgDiYvk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a732a63-79c3-4cc6-f94c-fb2c9e4236a1"
      },
      "source": [
        "! pip install nlpaug -q\n",
        "! pip install fairseq -q\n",
        "! pip install sacremoses -q\n",
        "! pip install fastBPE -q\n",
        "! pip install tqdm\n",
        "! pip install pyprind"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.41.1)\n",
            "Requirement already satisfied: pyprind in /usr/local/lib/python3.7/dist-packages (2.11.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynkQUYMKqmM3"
      },
      "source": [
        "import nlpaug.augmenter.char as nac\n",
        "import nlpaug.augmenter.word as naw\n",
        "import nlpaug.augmenter.sentence as nas\n",
        "import nlpaug.flow as naf\n",
        "\n",
        "from nlpaug.util import Action\n",
        "\n",
        "import pandas as pd\n",
        "import tqdm\n",
        "import pickle\n",
        "import pyprind \n",
        "import pandas as pd"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ciss4XUejbnQ",
        "outputId": "b3446678-f179-44ac-84d8-e792550e83ea"
      },
      "source": [
        "text = 'The quick brown fox jumps over the lazy dog .'\n",
        "print(text)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The quick brown fox jumps over the lazy dog .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FZWQrbWjWMa",
        "outputId": "891671fb-9944-4b4b-b763-b2119d66779b"
      },
      "source": [
        "\n",
        "aug = naw.SynonymAug(aug_src='wordnet')\n",
        "augmented_text = aug.augment(text)\n",
        "print(\"Original:\")\n",
        "print(text)\n",
        "print(\"Augmented Text:\")\n",
        "print(augmented_text)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:\n",
            "The quick brown fox jumps over the lazy dog .\n",
            "Augmented Text:\n",
            "The quick john brown fox rise terminated the lazy dog.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueX4n3EOrtaI",
        "outputId": "76fcdb99-1860-4039-bb66-8089b7430f5b"
      },
      "source": [
        "aug_del = naw.RandomWordAug()\n",
        "augmented_text = aug.augment(text)\n",
        "print(\"Original:\")\n",
        "print(text)\n",
        "print(\"Augmented Text:\")\n",
        "print(augmented_text)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:\n",
            "The quick brown fox jumps over the lazy dog .\n",
            "Augmented Text:\n",
            "The nimble brown dodger jumps over the lazy frank.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "gwaYsyTOcpBJ",
        "outputId": "c51625fe-3acb-400c-f2bd-5ea6f8c31128"
      },
      "source": [
        "import nlpaug.augmenter.word as naw\n",
        "\n",
        "text = 'i am sleepig on the floor'\n",
        "back_translation_aug = naw.BackTranslationAug(\n",
        "    from_model_name='transformer.wmt19.en-de', \n",
        "    to_model_name='transformer.wmt19.de-en'\n",
        ")\n",
        "augmented_text = back_translation_aug.augment(text)\n",
        "print(\"Original:\")\n",
        "print(text)\n",
        "print(\"Augmented Text:\")\n",
        "print(augmented_text)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_fairseq_master\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_fairseq_master\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'I sleep on the floor'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0PvJCfxrMST",
        "outputId": "48896092-4534-46a2-97c9-8ac09d64328d"
      },
      "source": [
        "aug = naf.Sometimes([\n",
        "    aug,back_translation_aug\n",
        "],aug_p=0.5, pipeline_p=0.5)\n",
        "\n",
        "aug.augment(text, n=4)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I sleep on the floor',\n",
              " 'm sleepy about the story',\n",
              " 'Number 53 sleeps on the floor']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BbZVXPrb-IO"
      },
      "source": [
        "dataset = pd.read_csv(\"/content/drive/MyDrive/END2.0/SentimentClassification/stanfordSentimentTreebank_dataset.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDhMxuQ_Cll8",
        "outputId": "f3cf0ef3-0649-46de-ebf2-64dc94f53d34"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5Ii53hXcF-H",
        "outputId": "2c7646b5-f6bf-4698-aeb2-3fc06f627656"
      },
      "source": [
        "dataset.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['num', 'text', 'label', 'split', 'label_distint'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKnwg7FlcJkN",
        "outputId": "25418f26-bad3-42de-d179-04ea290359c5"
      },
      "source": [
        "(dataset.label_distint.value_counts())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2    6240\n",
              "1    2175\n",
              "3    2139\n",
              "4     663\n",
              "0     638\n",
              "Name: label_distint, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V91ViW40dm9T"
      },
      "source": [
        "df_n=dataset[dataset.label_distint==1].reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9ruByEHbtcZ",
        "outputId": "7b329861-3661-4868-f7f0-04859182aba0"
      },
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def augment_text(df,samples=50,pr=0.2):\n",
        " \n",
        "    new_text=[]\n",
        "    for i in range(5):\n",
        "    ##selecting the minority class samples\n",
        "        df_n=df[df.label_distint==i].reset_index(drop=True)\n",
        "        ## data augmentation loop\n",
        "        print(np.random.randint(1,10,1))\n",
        "        for j in tqdm(np.random.randint(0,len(df_n),samples)):\n",
        "            \n",
        "                text = df_n.iloc[j]['text']\n",
        "                augmented_text = aug.augment(text)\n",
        "                new_text.append(augmented_text)\n",
        "        \n",
        "        \n",
        "        ## dataframe\n",
        "        new=pd.DataFrame({'text':new_text,'label_distint':i})\n",
        "        df=df.append(new).reset_index(drop=True)\n",
        "    return df\n",
        "   \n",
        "\n",
        "train_ = augment_text(dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[2]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "  2%|▏         | 1/50 [00:04<03:24,  4.17s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  4%|▍         | 2/50 [00:08<03:22,  4.23s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  6%|▌         | 3/50 [00:12<03:14,  4.14s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  8%|▊         | 4/50 [00:20<04:02,  5.26s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 10%|█         | 5/50 [00:26<04:02,  5.39s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 12%|█▏        | 6/50 [00:33<04:26,  6.07s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 14%|█▍        | 7/50 [00:43<05:12,  7.26s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 16%|█▌        | 8/50 [00:48<04:29,  6.42s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 18%|█▊        | 9/50 [00:53<04:07,  6.03s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 10/50 [00:56<03:31,  5.28s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 22%|██▏       | 11/50 [01:07<04:25,  6.80s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 24%|██▍       | 12/50 [01:20<05:27,  8.62s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 26%|██▌       | 13/50 [01:32<05:58,  9.68s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 28%|██▊       | 14/50 [01:37<04:59,  8.31s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 30%|███       | 15/50 [01:42<04:22,  7.50s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 32%|███▏      | 16/50 [01:54<04:57,  8.74s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 34%|███▍      | 17/50 [02:03<04:45,  8.65s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 36%|███▌      | 18/50 [02:09<04:13,  7.93s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 38%|███▊      | 19/50 [02:13<03:33,  6.87s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 20/50 [02:26<04:16,  8.57s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 42%|████▏     | 21/50 [02:34<04:03,  8.39s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 44%|████▍     | 22/50 [02:38<03:20,  7.16s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 46%|████▌     | 23/50 [02:38<02:16,  5.05s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 48%|████▊     | 24/50 [02:42<02:02,  4.70s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 50%|█████     | 25/50 [02:57<03:17,  7.89s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 52%|█████▏    | 26/50 [03:06<03:13,  8.07s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 54%|█████▍    | 27/50 [03:09<02:31,  6.57s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 56%|█████▌    | 28/50 [03:12<02:01,  5.51s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 58%|█████▊    | 29/50 [03:15<01:39,  4.75s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 30/50 [03:15<01:07,  3.36s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 62%|██████▏   | 31/50 [03:23<01:27,  4.61s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 64%|██████▍   | 32/50 [03:41<02:37,  8.75s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 66%|██████▌   | 33/50 [03:47<02:13,  7.84s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 68%|██████▊   | 34/50 [03:50<01:43,  6.50s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 70%|███████   | 35/50 [03:52<01:19,  5.27s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 72%|███████▏  | 36/50 [03:55<01:04,  4.60s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 74%|███████▍  | 37/50 [03:59<00:56,  4.38s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 76%|███████▌  | 38/50 [04:03<00:51,  4.25s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 78%|███████▊  | 39/50 [04:07<00:44,  4.04s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 40/50 [04:12<00:43,  4.30s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 82%|████████▏ | 41/50 [04:19<00:47,  5.32s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 84%|████████▍ | 42/50 [04:21<00:33,  4.24s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 86%|████████▌ | 43/50 [04:26<00:30,  4.29s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 88%|████████▊ | 44/50 [04:34<00:32,  5.49s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 90%|█████████ | 45/50 [04:39<00:26,  5.39s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 92%|█████████▏| 46/50 [04:49<00:26,  6.66s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 94%|█████████▍| 47/50 [04:57<00:21,  7.16s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 96%|█████████▌| 48/50 [04:57<00:10,  5.04s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 98%|█████████▊| 49/50 [05:01<00:04,  4.63s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 50/50 [05:15<00:00,  6.31s/it]\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[9]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "  2%|▏         | 1/50 [00:03<03:11,  3.92s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  4%|▍         | 2/50 [00:08<03:21,  4.20s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  6%|▌         | 3/50 [00:16<04:07,  5.26s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  8%|▊         | 4/50 [00:21<03:59,  5.22s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 10%|█         | 5/50 [00:21<02:45,  3.69s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 12%|█▏        | 6/50 [00:29<03:31,  4.80s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 14%|█▍        | 7/50 [00:29<02:25,  3.39s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 16%|█▌        | 8/50 [00:34<02:45,  3.94s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 18%|█▊        | 9/50 [00:51<05:18,  7.77s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 10/50 [00:56<04:42,  7.06s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 22%|██▏       | 11/50 [01:03<04:32,  6.99s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 24%|██▍       | 12/50 [01:09<04:11,  6.62s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 26%|██▌       | 13/50 [01:16<04:08,  6.71s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 28%|██▊       | 14/50 [01:16<02:50,  4.73s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 30%|███       | 15/50 [01:33<04:53,  8.39s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 32%|███▏      | 16/50 [01:39<04:20,  7.67s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 34%|███▍      | 17/50 [01:49<04:40,  8.50s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 36%|███▌      | 18/50 [01:59<04:50,  9.08s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 38%|███▊      | 19/50 [02:00<03:18,  6.39s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 20/50 [02:07<03:18,  6.61s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 42%|████▏     | 21/50 [02:10<02:44,  5.69s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 44%|████▍     | 22/50 [02:19<03:04,  6.58s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 46%|████▌     | 23/50 [02:26<03:02,  6.77s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 48%|████▊     | 24/50 [02:34<03:05,  7.13s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 50%|█████     | 25/50 [02:42<03:00,  7.23s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 52%|█████▏    | 26/50 [02:55<03:36,  9.03s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 54%|█████▍    | 27/50 [03:08<03:58, 10.35s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 56%|█████▌    | 28/50 [03:09<02:47,  7.62s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 58%|█████▊    | 29/50 [03:19<02:53,  8.29s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 30/50 [03:21<02:07,  6.37s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 62%|██████▏   | 31/50 [03:34<02:37,  8.31s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 64%|██████▍   | 32/50 [03:47<02:54,  9.70s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 66%|██████▌   | 33/50 [03:52<02:23,  8.42s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 68%|██████▊   | 34/50 [04:04<02:29,  9.37s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 70%|███████   | 35/50 [04:11<02:08,  8.57s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 72%|███████▏  | 36/50 [04:14<01:39,  7.10s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 74%|███████▍  | 37/50 [04:18<01:19,  6.11s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 76%|███████▌  | 38/50 [04:27<01:22,  6.91s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 78%|███████▊  | 39/50 [04:43<01:45,  9.55s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 40/50 [04:52<01:33,  9.39s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 82%|████████▏ | 41/50 [04:57<01:12,  8.05s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 84%|████████▍ | 42/50 [05:00<00:54,  6.75s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 86%|████████▌ | 43/50 [05:14<01:02,  8.86s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 90%|█████████ | 45/50 [05:20<00:35,  7.15s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 92%|█████████▏| 46/50 [05:23<00:22,  5.69s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 94%|█████████▍| 47/50 [05:36<00:24,  8.04s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 96%|█████████▌| 48/50 [05:51<00:19,  9.91s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 98%|█████████▊| 49/50 [05:56<00:08,  8.72s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 50/50 [06:05<00:00,  7.32s/it]\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[5]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "  2%|▏         | 1/50 [00:14<11:39, 14.28s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  4%|▍         | 2/50 [00:19<09:19, 11.65s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  6%|▌         | 3/50 [00:26<07:59, 10.20s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  8%|▊         | 4/50 [00:35<07:33,  9.85s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 10%|█         | 5/50 [00:43<06:55,  9.23s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 12%|█▏        | 6/50 [00:49<06:05,  8.30s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 14%|█▍        | 7/50 [00:55<05:27,  7.62s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 16%|█▌        | 8/50 [01:04<05:34,  7.96s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 18%|█▊        | 9/50 [01:04<03:49,  5.61s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 10/50 [01:06<02:58,  4.46s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 22%|██▏       | 11/50 [01:17<04:10,  6.42s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 24%|██▍       | 12/50 [01:22<03:48,  6.02s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 26%|██▌       | 13/50 [01:32<04:27,  7.23s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 28%|██▊       | 14/50 [01:41<04:38,  7.74s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 30%|███       | 15/50 [01:46<04:05,  7.02s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 32%|███▏      | 16/50 [01:53<03:58,  7.02s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 34%|███▍      | 17/50 [01:57<03:23,  6.15s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 36%|███▌      | 18/50 [02:03<03:08,  5.89s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 38%|███▊      | 19/50 [02:12<03:31,  6.81s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 20/50 [02:16<03:01,  6.04s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 42%|████▏     | 21/50 [02:32<04:21,  9.01s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 44%|████▍     | 22/50 [02:39<03:58,  8.52s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 46%|████▌     | 23/50 [02:44<03:22,  7.49s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 48%|████▊     | 24/50 [02:50<03:04,  7.08s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 50%|█████     | 25/50 [03:12<04:46, 11.45s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 52%|█████▏    | 26/50 [03:21<04:18, 10.75s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 54%|█████▍    | 27/50 [03:29<03:44,  9.76s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 56%|█████▌    | 28/50 [03:39<03:39,  9.99s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 58%|█████▊    | 29/50 [03:49<03:28,  9.93s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 30/50 [04:00<03:26, 10.30s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 62%|██████▏   | 31/50 [04:04<02:39,  8.39s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 64%|██████▍   | 32/50 [04:12<02:31,  8.41s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 66%|██████▌   | 33/50 [04:23<02:33,  9.02s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 68%|██████▊   | 34/50 [04:29<02:09,  8.06s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 70%|███████   | 35/50 [04:30<01:32,  6.17s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 72%|███████▏  | 36/50 [04:36<01:22,  5.91s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 74%|███████▍  | 37/50 [04:46<01:35,  7.34s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 76%|███████▌  | 38/50 [04:56<01:37,  8.09s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 78%|███████▊  | 39/50 [05:00<01:16,  6.92s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 40/50 [05:20<01:46, 10.63s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 82%|████████▏ | 41/50 [05:30<01:35, 10.59s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 84%|████████▍ | 42/50 [05:30<00:59,  7.44s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 86%|████████▌ | 43/50 [05:37<00:50,  7.21s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 88%|████████▊ | 44/50 [05:42<00:39,  6.56s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 90%|█████████ | 45/50 [06:00<00:50, 10.13s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 92%|█████████▏| 46/50 [06:05<00:34,  8.54s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 94%|█████████▍| 47/50 [06:09<00:21,  7.09s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 96%|█████████▌| 48/50 [06:24<00:18,  9.42s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 98%|█████████▊| 49/50 [06:32<00:09,  9.10s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 50/50 [06:35<00:00,  7.91s/it]\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[6]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "  2%|▏         | 1/50 [00:07<06:25,  7.87s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  4%|▍         | 2/50 [00:13<05:39,  7.07s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  6%|▌         | 3/50 [00:16<04:40,  5.97s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  8%|▊         | 4/50 [00:19<03:57,  5.15s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 10%|█         | 5/50 [00:24<03:50,  5.11s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 12%|█▏        | 6/50 [00:27<03:09,  4.30s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 14%|█▍        | 7/50 [00:37<04:26,  6.19s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 16%|█▌        | 8/50 [00:45<04:43,  6.74s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 18%|█▊        | 9/50 [00:53<04:44,  6.94s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 10/50 [01:00<04:41,  7.03s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 22%|██▏       | 11/50 [01:12<05:33,  8.55s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 24%|██▍       | 12/50 [01:17<04:43,  7.46s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 26%|██▌       | 13/50 [01:22<04:12,  6.82s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 28%|██▊       | 14/50 [01:29<04:07,  6.86s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 30%|███       | 15/50 [01:33<03:24,  5.84s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 32%|███▏      | 16/50 [01:38<03:15,  5.76s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 34%|███▍      | 17/50 [01:45<03:16,  5.95s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 36%|███▌      | 18/50 [01:56<04:02,  7.58s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 38%|███▊      | 19/50 [01:56<02:45,  5.34s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 20/50 [02:11<04:05,  8.19s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 42%|████▏     | 21/50 [02:12<02:57,  6.12s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 44%|████▍     | 22/50 [02:23<03:34,  7.65s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 46%|████▌     | 23/50 [02:35<03:55,  8.72s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 48%|████▊     | 24/50 [02:44<03:48,  8.79s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 50%|█████     | 25/50 [02:48<03:04,  7.36s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 52%|█████▏    | 26/50 [02:54<02:51,  7.16s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 54%|█████▍    | 27/50 [03:02<02:49,  7.36s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 56%|█████▌    | 28/50 [03:04<02:04,  5.65s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 58%|█████▊    | 29/50 [03:12<02:17,  6.54s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 30/50 [03:15<01:44,  5.23s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 62%|██████▏   | 31/50 [03:15<01:10,  3.69s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 64%|██████▍   | 32/50 [03:20<01:15,  4.20s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 66%|██████▌   | 33/50 [03:28<01:32,  5.45s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 68%|██████▊   | 34/50 [03:33<01:24,  5.25s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 70%|███████   | 35/50 [03:49<02:07,  8.52s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 72%|███████▏  | 36/50 [03:58<02:01,  8.68s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 74%|███████▍  | 37/50 [04:18<02:34, 11.91s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 76%|███████▌  | 38/50 [04:29<02:19, 11.63s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 78%|███████▊  | 39/50 [04:31<01:35,  8.65s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 40/50 [04:38<01:22,  8.30s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 82%|████████▏ | 41/50 [04:44<01:07,  7.51s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 84%|████████▍ | 42/50 [04:47<00:50,  6.26s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 86%|████████▌ | 43/50 [04:54<00:45,  6.47s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 88%|████████▊ | 44/50 [04:54<00:27,  4.56s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 90%|█████████ | 45/50 [05:04<00:30,  6.13s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 92%|█████████▏| 46/50 [05:04<00:17,  4.32s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 94%|█████████▍| 47/50 [05:10<00:14,  4.81s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 96%|█████████▌| 48/50 [05:10<00:06,  3.40s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 98%|█████████▊| 49/50 [05:18<00:04,  4.63s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 50/50 [05:24<00:00,  6.49s/it]\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[5]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "  2%|▏         | 1/50 [00:02<02:00,  2.47s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  4%|▍         | 2/50 [00:19<05:26,  6.81s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  6%|▌         | 3/50 [00:31<06:30,  8.30s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  8%|▊         | 4/50 [00:46<08:03, 10.51s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 10%|█         | 5/50 [00:54<07:15,  9.69s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 12%|█▏        | 6/50 [00:54<04:59,  6.82s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 14%|█▍        | 7/50 [01:12<07:20, 10.24s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 16%|█▌        | 8/50 [01:13<05:02,  7.20s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 18%|█▊        | 9/50 [01:23<05:31,  8.09s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|██        | 10/50 [01:27<04:34,  6.86s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 22%|██▏       | 11/50 [01:38<05:20,  8.21s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 24%|██▍       | 12/50 [01:42<04:17,  6.77s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 26%|██▌       | 13/50 [01:54<05:09,  8.36s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 28%|██▊       | 14/50 [01:56<03:59,  6.64s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 30%|███       | 15/50 [02:00<03:25,  5.88s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 32%|███▏      | 16/50 [02:04<02:53,  5.11s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 34%|███▍      | 17/50 [02:09<02:47,  5.06s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 36%|███▌      | 18/50 [02:17<03:14,  6.08s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 38%|███▊      | 19/50 [02:21<02:49,  5.45s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|████      | 20/50 [02:24<02:18,  4.61s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 42%|████▏     | 21/50 [02:34<03:01,  6.26s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 44%|████▍     | 22/50 [02:43<03:24,  7.30s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 46%|████▌     | 23/50 [02:46<02:36,  5.81s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 48%|████▊     | 24/50 [02:53<02:40,  6.16s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 50%|█████     | 25/50 [02:59<02:30,  6.04s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 52%|█████▏    | 26/50 [03:09<02:54,  7.29s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 54%|█████▍    | 27/50 [03:14<02:32,  6.62s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 56%|█████▌    | 28/50 [03:34<03:54, 10.66s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 58%|█████▊    | 29/50 [03:45<03:45, 10.76s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|██████    | 30/50 [03:59<03:53, 11.66s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 62%|██████▏   | 31/50 [04:08<03:26, 10.87s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 64%|██████▍   | 32/50 [04:14<02:51,  9.53s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 66%|██████▌   | 33/50 [04:26<02:53, 10.22s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 68%|██████▊   | 34/50 [04:34<02:32,  9.50s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 70%|███████   | 35/50 [04:41<02:13,  8.92s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 72%|███████▏  | 36/50 [04:45<01:41,  7.25s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 74%|███████▍  | 37/50 [04:46<01:11,  5.49s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 76%|███████▌  | 38/50 [05:00<01:34,  7.90s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 78%|███████▊  | 39/50 [05:07<01:25,  7.79s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|████████  | 40/50 [05:19<01:30,  9.02s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 82%|████████▏ | 41/50 [05:27<01:18,  8.74s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 84%|████████▍ | 42/50 [05:30<00:56,  7.10s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 86%|████████▌ | 43/50 [05:34<00:42,  6.04s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 88%|████████▊ | 44/50 [05:41<00:37,  6.24s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 90%|█████████ | 45/50 [05:51<00:37,  7.43s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 92%|█████████▏| 46/50 [05:51<00:20,  5.23s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 94%|█████████▍| 47/50 [06:00<00:18,  6.30s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 96%|█████████▌| 48/50 [06:06<00:12,  6.41s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 98%|█████████▊| 49/50 [06:17<00:07,  7.61s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|██████████| 50/50 [06:24<00:00,  7.68s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QERuRsOSnb9t"
      },
      "source": [
        "# train_.label_distint.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_FPo2cbU1xL",
        "outputId": "e07424b8-b976-4a19-9420-a5c05e89039b"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Jun  3 20:45:17 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P0    26W / 300W |      2MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVjc4RxrUmjJ",
        "outputId": "fcae55ad-61e2-46af-8ab8-0c138452aa4f"
      },
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your runtime has 13.6 gigabytes of available RAM\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2DYXVst0f3I",
        "outputId": "82835fae-228f-4249-b0b1-896ebbcd7d4d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYiRsFGD6iUC"
      },
      "source": [
        "# 0 TorchText"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tqdHKT3ARnl"
      },
      "source": [
        "import pandas as pd\n",
        "import pyprind\n",
        "import os, pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tp5IzBGsPGHs"
      },
      "source": [
        "## Dataset Preview\n",
        "\n",
        "Your first step to deep learning in NLP. We will be mostly using PyTorch. Just like torchvision, PyTorch provides an official library, torchtext, for handling text-processing pipelines. \n",
        "\n",
        "We will be using previous session tweet dataset. Let's just preview the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKH1qJWBvDsU"
      },
      "source": [
        "## Preparing Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WFgVCc03sbZ"
      },
      "source": [
        "dataset = pd.read_csv(\"/content/drive/MyDrive/END2.0/SentimentClassification/stanfordSentimentTreebank_dataset.csv\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJ6o_79ISSVb"
      },
      "source": [
        "## Defining Fields"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e63g08ijOrf7"
      },
      "source": [
        "Now we shall be defining LABEL as a LabelField, which is a subclass of Field that sets sequen tial to False (as it’s our numerical category class). TWEET is a standard Field object, where we have decided to use the spaCy tokenizer and convert all the text to lower‐ case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qk8IP4SK1Lrp",
        "outputId": "edb26d29-02a6-4e18-a763-5ce9acfa2a6e"
      },
      "source": [
        "# Import Library\n",
        "import random\n",
        "import torch, torchtext\n",
        "from torchtext.legacy import data\n",
        "import spacy\n",
        "sp = spacy.load('en_core_web_sm')\n",
        "\n",
        "\n",
        "# Manual Seed\n",
        "SEED = 43\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "\n",
        "all_stopwords = sp.Defaults.stop_words\n",
        "print(len(all_stopwords))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "326\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xd8MUyzfTE_J"
      },
      "source": [
        "train_df = dataset[dataset['split'] == 1].reset_index()\n",
        "test_df = dataset[dataset['split'] == 2].reset_index()\n",
        "val_df = dataset[dataset['split'] == 3].reset_index()\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6bKQax2Mf_U"
      },
      "source": [
        "Text = data.Field(sequential = True, tokenize = 'spacy', batch_first =True, include_lengths=True, stop_words = all_stopwords)\n",
        "Label = data.LabelField(tokenize ='spacy', is_target=True, batch_first =True, sequential =False)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX-lYIe_O7Vy"
      },
      "source": [
        "Having defined those fields, we now need to produce a list that maps them onto the list of rows that are in the CSV:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VawdWq36O6td"
      },
      "source": [
        "fields = [('text', Text),('labels',Label)]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbtZ-Ph2P1xL"
      },
      "source": [
        "Armed with our declared fields, lets convert from pandas to list to torchtext. We could also use TabularDataset to apply that definition to the CSV directly but showing an alternative approach too."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOXjVEOxIiDg"
      },
      "source": [
        "train_example = pickle.load( \n",
        "    open( \"/content/drive/MyDrive/END2.0/SentimentClassification/train_examples.pkl\", \"rb\" ) )\n",
        "val_example = pickle.load( \n",
        "    open( \"/content/drive/MyDrive/END2.0/SentimentClassification/val_examples.pkl\", \"rb\" ) )\n",
        "test_example = pickle.load( \n",
        "    open( \"/content/drive/MyDrive/END2.0/SentimentClassification/test_examples.pkl\", \"rb\" ) )\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hErcJfNT7Gj"
      },
      "source": [
        "# train_example = [data.Example.fromlist([train_df.text[i],train_df.label_distint[i]], fields) for i in range(train_df.shape[0])] \n",
        "# test_example = [data.Example.fromlist([test_df.text[i],test_df.label_distint[i]], fields) for i in range(test_df.shape[0])] \n",
        "# val_example = [data.Example.fromlist([val_df.text[i],val_df.label_distint[i]], fields) for i in range(val_df.shape[0])] "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nT-flpH-P1cd"
      },
      "source": [
        "train = data.Dataset(train_example, fields)\n",
        "test = data.Dataset(test_example, fields)\n",
        "valid = data.Dataset(val_example, fields)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykvsCGQMR6UD",
        "outputId": "82f49305-9e32-457b-9751-a86b06275057"
      },
      "source": [
        "len(train), len(test), len(valid)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8544, 2210, 1101)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kix8P2IKSBaV"
      },
      "source": [
        "An example from the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUpEOQruR9JL",
        "outputId": "823b7508-36e3-4286-c2c9-877b5ca66a32"
      },
      "source": [
        "vars(train.examples[1])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'labels': 2,\n",
              " 'text': ['The',\n",
              "  'gorgeously',\n",
              "  'elaborate',\n",
              "  'continuation',\n",
              "  '`',\n",
              "  '`',\n",
              "  'The',\n",
              "  'Lord',\n",
              "  'Rings',\n",
              "  \"''\",\n",
              "  'trilogy',\n",
              "  'huge',\n",
              "  'column',\n",
              "  'words',\n",
              "  'adequately',\n",
              "  'describe',\n",
              "  'co',\n",
              "  '-',\n",
              "  'writer\\\\/director',\n",
              "  'Peter',\n",
              "  'Jackson',\n",
              "  'expanded',\n",
              "  'vision',\n",
              "  'J.R.R.',\n",
              "  'Tolkien',\n",
              "  'Middle',\n",
              "  '-',\n",
              "  'earth',\n",
              "  '.']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKdllP3FST4N"
      },
      "source": [
        "## Building Vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuvWQ-SpSmSz"
      },
      "source": [
        "At this point we would have built a one-hot encoding of each word that is present in the dataset—a rather tedious process. Thankfully, torchtext will do this for us, and will also allow a max_size parameter to be passed in to limit the vocabu‐ lary to the most common words. This is normally done to prevent the construction of a huge, memory-hungry model. We don’t want our GPUs too overwhelmed, after all. \n",
        "\n",
        "Let’s limit the vocabulary to a maximum of 5000 words in our training set:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mx955u93SGeY"
      },
      "source": [
        "MAX_VOCAB_SIZE = 17_000\n",
        "\n",
        "Text.build_vocab(train, \n",
        "                 max_size = MAX_VOCAB_SIZE, \n",
        "                 vectors = \"glove.840B.300d\")\n",
        "Label.build_vocab(train)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCvJc2UqsR3q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58a36bd3-9c69-49c0-e462-b1d960cfd180"
      },
      "source": [
        "Text.vocab.vectors.shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16929, 300])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvyEeEjXTGhX"
      },
      "source": [
        "By default, torchtext will add two more special tokens, <unk> for unknown words and <pad>, a padding token that will be used to pad all our text to roughly the same size to help with efficient batching on the GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rA3tIESdcJdN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "104a3572-b147-4d49-8e70-9169702e2d8e"
      },
      "source": [
        "print('Size of input vocab : ', len(Text.vocab))\n",
        "print('Size of label vocab : ', len(Label.vocab))\n",
        "print('Top 10 words appreared repeatedly :', list(Text.vocab.freqs.most_common(10)))\n",
        "print('Labels : ', Label.vocab.stoi)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of input vocab :  16929\n",
            "Size of label vocab :  5\n",
            "Top 10 words appreared repeatedly : [('.', 8041), (',', 7131), ('-', 2737), ('The', 1265), ('film', 1156), ('movie', 999), ('A', 882), ('`', 788), ('It', 647), ('...', 643)]\n",
            "Labels :  defaultdict(None, {2: 0, 1: 1, 3: 2, 4: 3, 0: 4})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04s7XvPI0xeQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6360d4f-9ac9-4743-81bd-dffd89e1416b"
      },
      "source": [
        "print(Text.vocab.itos[:10])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<unk>', '<pad>', '.', ',', '-', 'The', 'film', 'movie', 'A', '`']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwjD2-ebTeUX"
      },
      "source": [
        "**Lots of stopwords!!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLWW221gTpNs"
      },
      "source": [
        "Now we need to create a data loader to feed into our training loop. Torchtext provides the BucketIterator method that will produce what it calls a Batch, which is almost, but not quite, like the data loader we used on images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQqMhMoDUDmn"
      },
      "source": [
        "But at first declare the device we are using."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zfo2QhGJUK4l"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zK2ORoqdTNsM"
      },
      "source": [
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits((train, valid, test), batch_size = 64, \n",
        "                                                            sort_key = lambda x: len(x.text),\n",
        "                                                            sort_within_batch=True, device = device)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sseGRHmf1G6y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2144b70f-f053-408d-cc2e-c63a7c7c31e0"
      },
      "source": [
        "print('Train')\n",
        "for batch in train_iterator:\n",
        "    print(f'Text matrix size: {batch.text[0].size()}')\n",
        "    print(f'Target vector size: {batch.labels.size()}')\n",
        "    break\n",
        "    \n",
        "print('\\nValid:')\n",
        "for batch in valid_iterator:\n",
        "    print(f'Text matrix size: {batch.text[0].size()}')\n",
        "    print(f'Target vector size: {batch.labels.size()}')\n",
        "    break\n",
        "    \n",
        "print('\\nTest:')\n",
        "for batch in test_iterator:\n",
        "    print(f'Text matrix size: {batch.text[0].size()}')\n",
        "    print(f'Target vector size: {batch.labels.size()}')\n",
        "    break"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train\n",
            "Text matrix size: torch.Size([64, 22])\n",
            "Target vector size: torch.Size([64])\n",
            "\n",
            "Valid:\n",
            "Text matrix size: torch.Size([64, 5])\n",
            "Target vector size: torch.Size([64])\n",
            "\n",
            "Test:\n",
            "Text matrix size: torch.Size([64, 4])\n",
            "Target vector size: torch.Size([64])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gg7gTFQO4fby"
      },
      "source": [
        "Save the vocabulary for later use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niE9Cc6-2bD_"
      },
      "source": [
        "\n",
        "with open('/content/drive/MyDrive/END2.0/SentimentClassification/tokenizer.pkl', 'wb') as tokens: \n",
        "  pickle.dump(Text.vocab.stoi, tokens)\n",
        "\n",
        "# with open('/content/drive/MyDrive/END2.0/SentimentClassification/train_examples.pkl', 'wb') as tokens: \n",
        "#     pickle.dump(train_example, tokens)\n",
        "\n",
        "# with open('/content/drive/MyDrive/END2.0/SentimentClassification/val_examples.pkl', 'wb') as tokens: \n",
        "#     pickle.dump(val_example, tokens)\n",
        "\n",
        "# with open('/content/drive/MyDrive/END2.0/SentimentClassification/test_examples.pkl', 'wb') as tokens: \n",
        "#     pickle.dump(test_example, tokens)\n",
        "\n",
        "\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AbsQwqkVyAy"
      },
      "source": [
        "## Defining Our Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4PED4HJWH4t"
      },
      "source": [
        "We use the Embedding and LSTM modules in PyTorch to build a simple model for classifying tweets.\n",
        "\n",
        "In this model we create three layers. \n",
        "1. First, the words in our tweets are pushed into an Embedding layer, which we have established as a 300-dimensional vector embedding. \n",
        "2. That’s then fed into a 2 stacked-LSTMs with 100 hidden features (again, we’re compressing down from the 300-dimensional input like we did with images). We are using 2 LSTMs for using the dropout.\n",
        "3. Finally, the output of the LSTM (the final hidden state after processing the incoming tweet) is pushed through a standard fully connected layer with three outputs to correspond to our three possible classes (negative, positive, or neutral)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43pVRccMT0bT"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class classifier(nn.Module):\n",
        "    \n",
        "    # Define all the layers used in model\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout):\n",
        "        \n",
        "        super().__init__()          \n",
        "        \n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        \n",
        "        # LSTM layer\n",
        "        self.encoder = nn.LSTM(embedding_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           dropout=dropout,\n",
        "                           batch_first=True,\n",
        "                           bidirectional= True)\n",
        "        # try using nn.GRU or nn.RNN here and compare their performances\n",
        "        # try bidirectional and compare their performances\n",
        "        \n",
        "        # Dense layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "    def forward(self, text, text_lengths):\n",
        "        \n",
        "        # text = [batch size, sent_length]\n",
        "        embedded = self.embedding(text)\n",
        "        # embedded = [batch size, sent_len, emb dim]\n",
        "      \n",
        "        # packed sequence\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu(), batch_first=True)\n",
        "        \n",
        "        # packed_output ,hidden = self.encoder(packed_embedded)\n",
        "        \n",
        "        packed_output, (hidden, cell) = self.encoder(packed_embedded)\n",
        "        #hidden = [batch size, num layers * num directions,hid dim]\n",
        "        #cell = [batch size, num layers * num directions,hid dim]\n",
        "        # Hidden = [batch size, hid dim * num directions]\n",
        "        dense_outputs = self.fc(hidden)   \n",
        "        \n",
        "        # Final activation function softmax\n",
        "        output = F.softmax(dense_outputs[0], dim=1)\n",
        "        # print (output)\n",
        "        return output"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwBoGE_X_Fl8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e95ac12-ba2b-47cc-e3a2-2ec69f54dd5c"
      },
      "source": [
        "# Define hyperparameters\n",
        "size_of_vocab = len(Text.vocab)\n",
        "embedding_dim = 300\n",
        "num_hidden_nodes = 16\n",
        "num_output_nodes = 5\n",
        "num_layers = 1\n",
        "dropout = 0.5\n",
        "PAD_IDX = Text.vocab.stoi[Text.pad_token]\n",
        "# Instantiate the model\n",
        "model = classifier(size_of_vocab, embedding_dim, num_hidden_nodes, num_output_nodes, num_layers, dropout = dropout)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:63: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPbLamPYsjC5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62ce407c-1b12-4b60-e911-88a1ba1bdf88"
      },
      "source": [
        "pretrained_embeddings = Text.vocab.vectors\n",
        "\n",
        "print(pretrained_embeddings.shape)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([16929, 300])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Z1V6koJswUV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6a6e824-47ca-48f9-848a-da7e745f37d4"
      },
      "source": [
        "# model.embedding.weight.data = pretrained_embeddings.cuda()\n",
        "model.embedding.weight.data.copy_(pretrained_embeddings)\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0120,  0.2075, -0.1258,  ...,  0.1387, -0.3605, -0.0350],\n",
              "        ...,\n",
              "        [ 0.0164, -0.1268,  0.1124,  ..., -0.0723,  0.4662, -0.3872],\n",
              "        [-0.0592,  0.1091, -0.2313,  ...,  0.0914,  0.6806, -0.4423],\n",
              "        [-0.3185, -0.0888,  0.0675,  ..., -0.2871,  0.6534, -0.5551]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4Cu1uO5mYe7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efa73a07-8fd6-429d-913e-ea62420d9b42"
      },
      "source": [
        "\n",
        "model.embedding.weight.data.shape"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16929, 300])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2MpDvknl7Xl"
      },
      "source": [
        "UNK_IDX = Text.vocab.stoi[Text.unk_token]\n",
        "\n",
        "model.embedding.weight.data[UNK_IDX] = torch.zeros(embedding_dim)\n",
        "model.embedding.weight.data[PAD_IDX] = torch.zeros(embedding_dim)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-pOMqzJ3eTv",
        "outputId": "33789dc6-f12d-4433-cd73-083c519a99e9"
      },
      "source": [
        "print(model)\n",
        "\n",
        "#No. of trianable parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    \n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "classifier(\n",
            "  (embedding): Embedding(16929, 300)\n",
            "  (encoder): LSTM(300, 16, batch_first=True, dropout=0.5, bidirectional=True)\n",
            "  (fc): Linear(in_features=16, out_features=5, bias=True)\n",
            ")\n",
            "The model has 5,119,489 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXajorf5Xz7t"
      },
      "source": [
        "## Model Training and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrE9RpMtZ1Vs"
      },
      "source": [
        "First define the optimizer and loss functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-u86JWdlXvu5"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# define optimizer and loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# define metric\n",
        "def binary_accuracy(preds, y):\n",
        "    #round predictions to the closest integer\n",
        "    _, predictions = torch.max(preds, 1)\n",
        "    \n",
        "    correct = (predictions == y).float() \n",
        "    # print (predictions, y)\n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n",
        "\n",
        "def categorical_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "    top_pred = preds.argmax(1, keepdim = True)\n",
        "    # y_pred_tags = preds.argmax(dim=1) \n",
        "    # print(preds)\n",
        "    # print(top_pred, y)\n",
        "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
        "    acc = correct.float() / y.shape[0]\n",
        "    return acc    \n",
        "# push to cuda if available\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VCJtNb3Zt8w"
      },
      "source": [
        "The main thing to be aware of in this new training loop is that we have to reference `batch.tweets` and `batch.labels` to get the particular fields we’re interested in; they don’t fall out quite as nicely from the enumerator as they do in torchvision."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WjEPLKsAiS_"
      },
      "source": [
        "**Training Loop**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDWNnGK3Y5oJ"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    # initialize every epoch \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    epoch_f1 = 0\n",
        "    # set the model in training phase\n",
        "    model.train()  \n",
        "    bar = pyprind.ProgBar(len(iterator), bar_char='█')\n",
        "    for batch in iterator:\n",
        "        \n",
        "        # resets the gradients after every batch\n",
        "        optimizer.zero_grad()   \n",
        "        \n",
        "        # retrieve text and no. of words\n",
        "        text, text_lengths = batch.text   \n",
        "        \n",
        "        # convert to 1D tensor\n",
        "        predictions = model(text, text_lengths).squeeze()  \n",
        "        # print(list(predictions))\n",
        "        # compute the loss\n",
        "        loss = criterion(predictions, batch.labels)        \n",
        "        \n",
        "        # compute the category accuracy\n",
        "        acc = categorical_accuracy(predictions, batch.labels)   \n",
        "        true_y = F.one_hot(batch.labels, 5).to('cpu')\n",
        " \n",
        "        f1_scr = f1_score(true_y, predictions.data.to('cpu') > 0.5, average=\"samples\")\n",
        "        # backpropage the loss and compute the gradients\n",
        "        # print(epoch_f1)\n",
        "        loss.backward()       \n",
        "        \n",
        "        # update the weights\n",
        "        optimizer.step()      \n",
        "        \n",
        "        # loss and accuracy\n",
        "        epoch_loss += loss.item()  \n",
        "        epoch_acc += acc.item()  \n",
        "        epoch_f1 += f1_scr \n",
        "        bar.update()\n",
        "        # break\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_f1/ len(iterator)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZcHhkkvAsCt"
      },
      "source": [
        "**Evaluation Loop**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHEe-zSVAriL"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    # initialize every epoch\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    epoch_f1 = 0\n",
        "\n",
        "    # deactivating dropout layers\n",
        "    model.eval()\n",
        "    \n",
        "    # deactivates autograd\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "        \n",
        "            # retrieve text and no. of words\n",
        "            text, text_lengths = batch.text\n",
        "            \n",
        "            # convert to 1d tensor\n",
        "            predictions = model(text, text_lengths).squeeze()\n",
        "            \n",
        "            # compute loss and accuracy\n",
        "            loss = criterion(predictions, batch.labels)\n",
        "            acc = categorical_accuracy(predictions, batch.labels)\n",
        "            true_y = F.one_hot(batch.labels, 5).to('cpu')\n",
        " \n",
        "            f1_scr = f1_score(true_y, predictions.data.to('cpu') > 0.5, average=\"samples\")\n",
        "            # keep track of loss and accuracy\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "            epoch_f1 += f1_scr\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_f1/ len(iterator)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6LJFW7HaJoV"
      },
      "source": [
        "**Let's Train and Evaluate**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tq330XlnaEU9",
        "outputId": "fcf19955-34de-442a-a63d-f4b28b416ac3"
      },
      "source": [
        "from sklearn.metrics import classification_report, f1_score\n",
        "N_EPOCHS = 5\n",
        "train_acc_list = []\n",
        "val_acc_list = []\n",
        "train_loss_list = []\n",
        "val_loss_list = []\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "     \n",
        "    # train the model\n",
        "    train_loss, train_acc, train_f1 = train(model, train_iterator, optimizer, criterion)\n",
        "    train_acc_list.append(train_acc)\n",
        "    train_loss_list.append(train_loss)\n",
        "    # evaluate the model\n",
        "    valid_loss, valid_acc, val_f1 = evaluate(model, valid_iterator, criterion)\n",
        "    val_loss_list.append(valid_loss)\n",
        "    val_acc_list.append(valid_acc)\n",
        "    # save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    print(f'\\tEPOCH: {epoch}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% |  F1 Score: {train_f1}')\n",
        "\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}% |  F1 Score: {val_f1}')\n",
        "    # break"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
            "Total time elapsed: 00:00:01\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tEPOCH: 0\n",
            "\tTrain Loss: 1.554 | Train Acc: 45.17% |  F1 Score: 0.0061800373134328354\n",
            "\t Val. Loss: 1.522 |  Val. Acc: 45.71% |  F1 Score: 0.21948450854700854\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
            "Total time elapsed: 00:00:01\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tEPOCH: 1\n",
            "\tTrain Loss: 1.445 | Train Acc: 53.54% |  F1 Score: 0.4982509328358209\n",
            "\t Val. Loss: 1.469 |  Val. Acc: 45.71% |  F1 Score: 0.444911858974359\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
            "Total time elapsed: 00:00:01\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tEPOCH: 2\n",
            "\tTrain Loss: 1.397 | Train Acc: 53.63% |  F1 Score: 0.5306669776119403\n",
            "\t Val. Loss: 1.456 |  Val. Acc: 45.71% |  F1 Score: 0.45706463675213677\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
            "Total time elapsed: 00:00:01\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tEPOCH: 3\n",
            "\tTrain Loss: 1.381 | Train Acc: 53.68% |  F1 Score: 0.5356809701492538\n",
            "\t Val. Loss: 1.451 |  Val. Acc: 45.71% |  F1 Score: 0.45706463675213677\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
            "Total time elapsed: 00:00:01\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tEPOCH: 4\n",
            "\tTrain Loss: 1.377 | Train Acc: 53.64% |  F1 Score: 0.5359141791044776\n",
            "\t Val. Loss: 1.450 |  Val. Acc: 45.71% |  F1 Score: 0.45706463675213677\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "29gt-1Vxn8ij",
        "outputId": "88074fa1-5d5f-45ca-d67c-a10c5febaac6"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "loss_train = train_acc_list\n",
        "loss_val = val_acc_list\n",
        "epochs = range(0,N_EPOCHS)\n",
        "plt.plot(epochs, loss_train, 'g', label='Training accuracy')\n",
        "plt.plot(epochs, loss_val, 'b', label='validation accuracy')\n",
        "plt.title('Training and Validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZgzB0ZkHVTI"
      },
      "source": [
        "## Model Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QU1OckfuXFh",
        "outputId": "c8ae7a42-6c2a-4dd6-8e5b-93b9483e1059"
      },
      "source": [
        "path='./saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path))\n",
        "model.eval()\n",
        "test_loss, test_acc,test_f1 = evaluate(model, test_iterator, criterion)\n",
        "print(f'\\t Test. Loss: {test_loss:.3f} |  Test. Acc: {test_acc*100:.2f}% |  Test. F1: {test_f1}\\n')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t Test. Loss: 1.389 |  Test. Acc: 52.27% |  Test. F1: 0.5227153361344539\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZZfnWo0abRx"
      },
      "source": [
        "#load weights and tokenizer\n",
        "\n",
        "path='./saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path));\n",
        "model.eval();\n",
        "tokenizer_file = open('/content/drive/MyDrive/END2.0/SentimentClassification/tokenizer.pkl', 'rb')\n",
        "tokenizer = pickle.load(tokenizer_file)\n",
        "\n",
        "#inference \n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "def classify_tweet(tweet):\n",
        "    # {2: 0, 1: 1, 3: 2, 4: 3, 0: 4}  very negative, negative, neutral, positive, very positive\n",
        "    categories = {0:\"neutral\", 1:\"negative\",2: \"positive\", 3:\"very positive\", 4:\"very negative\" }\n",
        "    \n",
        "    # tokenize the tweet \n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(tweet)] \n",
        "    # convert to integer sequence using predefined tokenizer dictionary\n",
        "    indexed = [tokenizer[t] for t in tokenized]        \n",
        "    # compute no. of words        \n",
        "    length = [len(indexed)]\n",
        "    # convert to tensor                                    \n",
        "    tensor = torch.LongTensor(indexed).to(device)   \n",
        "    # reshape in form of batch, no. of words           \n",
        "    tensor = tensor.unsqueeze(1).T  \n",
        "    # convert to tensor                          \n",
        "    length_tensor = torch.LongTensor(length)\n",
        "    # Get the model prediction                  \n",
        "    prediction = model(tensor, length_tensor)\n",
        "\n",
        "    _, pred = torch.max(prediction, 1) \n",
        "    \n",
        "    return categories[pred.item()]"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTkHLEipIlM9",
        "outputId": "d5a72e45-c269-4f68-fee0-369d4d9f0d80"
      },
      "source": [
        "dataset = pd.read_csv(\"/content/drive/MyDrive/END2.0/SentimentClassification/stanfordSentimentTreebank_dataset.csv\")\n",
        "test_df = dataset[dataset['split'] == 2].reset_index()\n",
        "import numpy as np\n",
        "categories = {0:\"very negative\", 1:\"negative\",2: \"neutral\", 3:\"positive\", 4:\"very positive\" }\n",
        "trail = 9\n",
        "list_rand = np.random.randint(1, test_df.shape[0], 10)\n",
        "print(list_rand)\n",
        "for index, i in test_df.iloc[list_rand].iterrows():\n",
        "    # print(i)\n",
        "    print(\"Sentence: \", i.text)\n",
        "    print(\"True Label: \", categories[i.label_distint])\n",
        "    print(\"Predicted: \" ,classify_tweet(i.text))\n",
        "    print(\"-\"*25)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2004  362  669 1717 1301 1093 2023 1210 1902   92]\n",
            "Sentence:  You would be better off investing in the worthy EMI recording that serves as the soundtrack , or the home video of the 1992 Malfitano-Domingo production .\n",
            "\n",
            "True Label:  neutral\n",
            "Predicted:  neutral\n",
            "-------------------------\n",
            "Sentence:  A tender and touching drama , based on the true story of a troubled African-American 's quest to come to terms with his origins , reveals the yearning we all have in our hearts for acceptance within the family circle .\n",
            "\n",
            "True Label:  positive\n",
            "Predicted:  neutral\n",
            "-------------------------\n",
            "Sentence:  Though the plot is predictable , the movie never feels formulaic , because the attention is on the nuances of the emotional development of the delicate characters .\n",
            "\n",
            "True Label:  neutral\n",
            "Predicted:  neutral\n",
            "-------------------------\n",
            "Sentence:  The movie is too cute to take itself too seriously , but it still feels like it was made by some very stoned college students .\n",
            "\n",
            "True Label:  neutral\n",
            "Predicted:  neutral\n",
            "-------------------------\n",
            "Sentence:  Human Nature initially succeeds by allowing itself to go crazy , but ultimately fails by spinning out of control .\n",
            "\n",
            "True Label:  positive\n",
            "Predicted:  neutral\n",
            "-------------------------\n",
            "Sentence:  The overall effect is so completely inane that one would have to be mighty bored to even think of staying with this for more than , say , ten ... make that three minutes .\n",
            "\n",
            "True Label:  neutral\n",
            "Predicted:  neutral\n",
            "-------------------------\n",
            "Sentence:  Going to the website may be just as fun -LRB- and scary -RRB- as going to the film .\n",
            "\n",
            "True Label:  neutral\n",
            "Predicted:  neutral\n",
            "-------------------------\n",
            "Sentence:  I hated every minute of it .\n",
            "\n",
            "True Label:  neutral\n",
            "Predicted:  neutral\n",
            "-------------------------\n",
            "Sentence:  Formula 51 is so trite that even Yu 's high-energy action stylings ca n't break through the stupor .\n",
            "\n",
            "True Label:  neutral\n",
            "Predicted:  neutral\n",
            "-------------------------\n",
            "Sentence:  In a normal screen process , these bromides would be barely enough to sustain an interstitial program on the Discovery Channel .\n",
            "\n",
            "True Label:  positive\n",
            "Predicted:  neutral\n",
            "-------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVjCuKK_LVEF"
      },
      "source": [
        "## Discussion on Data Augmentation Techniques \n",
        "\n",
        "You might wonder exactly how you can augment text data. After all, you can’t really flip it horizontally as you can an image! :D \n",
        "\n",
        "In contrast to data augmentation in images, augmentation techniques on data is very specific to final product you are building. As its general usage on any type of textual data doesn't provides a significant performance boost, that's why unlike torchvision, torchtext doesn’t offer a augmentation pipeline. Due to powerful models as transformers, augmentation tecnhiques are not so preferred now-a-days. But its better to know about some techniques with text that will provide your model with a little more information for training. \n",
        "\n",
        "### Synonym Replacement\n",
        "\n",
        "First, you could replace words in the sentence with synonyms, like so:\n",
        "\n",
        "    The dog slept on the mat\n",
        "\n",
        "could become\n",
        "\n",
        "    The dog slept on the rug\n",
        "\n",
        "Aside from the dog's insistence that a rug is much softer than a mat, the meaning of the sentence hasn’t changed. But mat and rug will be mapped to different indices in the vocabulary, so the model will learn that the two sentences map to the same label, and hopefully that there’s a connection between those two words, as everything else in the sentences is the same."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_uEfWJpL6Nq"
      },
      "source": [
        "### Random Insertion\n",
        "A random insertion technique looks at a sentence and then randomly inserts synonyms of existing non-stopwords into the sentence n times. Assuming you have a way of getting a synonym of a word and a way of eliminating stopwords (common words such as and, it, the, etc.), shown, but not implemented, in this function via get_synonyms() and get_stopwords(), an implementation of this would be as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Alm5D7WIvAC"
      },
      "source": [
        "def random_insertion(sentence, n): \n",
        "    words = remove_stopwords(sentence) \n",
        "    for _ in range(n):\n",
        "        new_synonym = get_synonyms(random.choice(words))\n",
        "        sentence.insert(randrange(len(sentence)+1), new_synonym) \n",
        "    return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqLWzwJ3Mm8h"
      },
      "source": [
        "## Random Deletion\n",
        "As the name suggests, random deletion deletes words from a sentence. Given a probability parameter p, it will go through the sentence and decide whether to delete a word or not based on that random probability. Consider of it as pixel dropouts while treating images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7Dz7JJfMqyC"
      },
      "source": [
        "def random_deletion(words, p=0.5): \n",
        "    if len(words) == 1: # return if single word\n",
        "        return words\n",
        "    remaining = list(filter(lambda x: random.uniform(0,1) > p,words)) \n",
        "    if len(remaining) == 0: # if not left, sample a random word\n",
        "        return [random.choice(words)] \n",
        "    else:\n",
        "        return remaining"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOIbi5WzO5OU"
      },
      "source": [
        "### Random Swap\n",
        "The random swap augmentation takes a sentence and then swaps words within it n times, with each iteration working on the previously swapped sentence. Here we sample two random numbers based on the length of the sentence, and then just keep swapping until we hit n."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnkbG15HO3Yj"
      },
      "source": [
        "def random_swap(sentence, n=5): \n",
        "    length = range(len(sentence)) \n",
        "    for _ in range(n):\n",
        "        idx1, idx2 = random.sample(length, 2)\n",
        "        sentence[idx1], sentence[idx2] = sentence[idx2], sentence[idx1] \n",
        "    return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "599NpwfMR5Vm"
      },
      "source": [
        "For more on this please go through this [paper](https://arxiv.org/pdf/1901.11196.pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5aeKuNCRGip"
      },
      "source": [
        "### Back Translation\n",
        "\n",
        "Another popular approach for augmenting text datasets is back translation. This involves translating a sentence from our target language into one or more other languages and then translating all of them back to the original language. We can use the Python library googletrans for this purpose. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHhNBbYrRXNy"
      },
      "source": [
        "import random\n",
        "import googletrans\n",
        "import googletrans.Translator\n",
        "\n",
        "translator = Translator()\n",
        "sentence = ['']\n",
        "\n",
        "available_langs = list(googletrans.LANGUAGES.keys()) \n",
        "trans_lang = random.choice(available_langs) \n",
        "print(f\"Translating to {googletrans.LANGUAGES[trans_lang]}\")\n",
        "\n",
        "translations = translator.translate(sentence, dest=trans_lang) \n",
        "t_text = [t.text for t in translations]\n",
        "print(t_text)\n",
        "\n",
        "translations_en_random = translator.translate(t_text, src=trans_lang, dest='en') \n",
        "en_text = [t.text for t in translations_en_random]\n",
        "print(en_text)"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiscgzPjAQRI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2L6Uk-60dk60"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}