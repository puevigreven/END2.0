{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/puevigreven/END2.0/blob/main/HW3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "3o7-sn4FHVYy"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "sjp0oD3tHzVZ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "tITC02cWH5_x"
   },
   "outputs": [],
   "source": [
    "### Run this cell\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "\n",
    "# Download and load the training data\n",
    "train_set = datasets.MNIST('drive/My Drive/END2.0/MNIST_data/', download=True, train=True, transform=transform)\n",
    "val_set = datasets.MNIST('drive/My Drive/END2.0/MNIST_data/', download=True, train=False, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tAI_CI3eIApA",
    "outputId": "7613c158-cb60-4294-f106-9f0a70a08b21"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = next(iter(train_set))\n",
    "len(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "VIufTYPcI8h2",
    "outputId": "412cea27-05d7-4ffb-eb2c-3bd1c688a95b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 248,
       "width": 251
      },
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(sample[0].numpy().squeeze(), cmap='gray_r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZUQf_vZh-Fz"
   },
   "source": [
    "### Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "GPbQv2EpJSsB"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class MNIST_Random(Dataset):\n",
    "  def __init__(self, train_set):\n",
    "    # train_set = datasets.MNIST('drive/My Drive/END2.0/MNIST_data/', download=True, train=True, transform=transform)\n",
    "    self.data = train_set.data\n",
    "    self.data = self.data.unsqueeze(1)\n",
    "    self.data = self.data.float()\n",
    "    \n",
    "    self.targets = train_set.targets #mnist image true value \n",
    "    self.random_input = torch.randint(0,10, (len(train_set.data),)) # random input \n",
    "    self.label_2 = self.targets + self.random_input # sum of random input\n",
    "    \n",
    "  def __getitem__(self, index):\n",
    "    img, targets, random_input, sum = self.data[index], int(self.targets[index]), int(self.random_input[index]), int(self.label_2[index])\n",
    "\n",
    "    return img, targets, random_input, sum\n",
    "  def __len__(self):\n",
    "    return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LPrrO4HwiRPl"
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "id": "pC-KEflZt7wS"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.fc3 = nn.Linear(20,20)\n",
    "        self.fc4 = nn.Linear(20,19)\n",
    "    def forward(self, x, random):\n",
    "        random = F.one_hot(random, num_classes=10)\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        mnist_output = self.fc2(x)\n",
    "        # print(mnist_output.shape,random.shape )\n",
    "        concat = torch.cat([mnist_output, random], dim=-1) # Concatinating the mnist output and random input\n",
    "        x = self.fc3(concat)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc4(concat)\n",
    "        x = F.relu(x)\n",
    "        # output = F.log_softmax(mnist_output, dim=1)\n",
    "        # add_out =  F.log_softmax(x, dim=1)\n",
    "        return mnist_output, x\n",
    "\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch, log_interval, dry_run):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target, random_number, rand_sum) in enumerate(train_loader):\n",
    "        \n",
    "        data, target = data.to(device), target.to(device)\n",
    "        random_number, rand_sum = random_number.to(device), rand_sum.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output, add_out = model(data,random_number)\n",
    "        loss = nn.CrossEntropyLoss()\n",
    "        l1 = loss(output, target)\n",
    "        l2 = loss(add_out, rand_sum)\n",
    "        loss = l1 + l2\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            if dry_run:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "id": "ZhpSJYp0t32L"
   },
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target , random_number, rand_sum in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            random_number, rand_sum = random_number.to(device), rand_sum.to(device)\n",
    "\n",
    "            output, add_out = model(data,random_number)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "# main function to run the training \n",
    "def main():\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    seed = 121\n",
    "    batch_size = 32\n",
    "    test_batch_size = 64\n",
    "    lr = 0.001\n",
    "    gamma = 0.7\n",
    "    epochs = 15\n",
    "    log_interval = 100\n",
    "    dry_run = False\n",
    "    save_model = False\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    train_kwargs = {'batch_size': batch_size}\n",
    "    test_kwargs = {'batch_size': test_batch_size}\n",
    "    if use_cuda:\n",
    "        cuda_kwargs = {'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True}\n",
    "        train_kwargs.update(cuda_kwargs)\n",
    "        test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "    dataset1 = datasets.MNIST('drive/My Drive/END2.0/MNIST_data/', train=True, download=True,\n",
    "                       transform=transform)\n",
    "    dataset2 = datasets.MNIST('drive/My Drive/END2.0/MNIST_data/', train=False,\n",
    "                       transform=transform)\n",
    "    dataset1 = MNIST_Random(dataset1)\n",
    "    dataset2 = MNIST_Random(dataset2)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "\n",
    "    model = Net().to(device)\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=lr)\n",
    "\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train( model, device, train_loader, optimizer, epoch, log_interval, dry_run)\n",
    "        test(model, device, test_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "    if save_model:\n",
    "        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uNe1dObDv4_0",
    "outputId": "0e924da5-b106-4685-894d-36b48164282b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 18.122763\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 10.193660\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 6.671815\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 5.465380\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 4.935543\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 5.296899\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 4.771879\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 4.646603\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 5.093919\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 4.490909\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 4.785513\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 5.017439\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 4.620586\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 4.305861\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 4.146877\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 4.618503\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 4.494646\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 4.503683\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 4.500049\n",
      "\n",
      "Test set: Average loss: -2.5165, Accuracy: 7936/10000 (79%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 4.576081\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 4.143214\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 4.516819\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 4.364698\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 3.816038\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 4.804969\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 4.250186\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 4.023621\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 3.909662\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 4.282988\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 3.978449\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 4.494811\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 4.424699\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 4.515723\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 4.301325\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 3.576724\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 4.214731\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 4.359541\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 3.844002\n",
      "\n",
      "Test set: Average loss: -3.4359, Accuracy: 8547/10000 (85%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 4.171904\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 3.891325\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 4.373222\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 4.050827\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 4.349957\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 4.468716\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 3.806951\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 3.892641\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 4.033205\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 3.914741\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 4.126940\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 4.288429\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 4.504932\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 4.066735\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 4.108382\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 4.271751\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 4.420452\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 3.985962\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 4.614625\n",
      "\n",
      "Test set: Average loss: -3.8415, Accuracy: 8750/10000 (88%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 3.949430\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 4.354843\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 4.374903\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 4.076168\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 4.419128\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 3.873377\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 3.796365\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 3.875438\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 3.614665\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 3.899329\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 4.211699\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 4.247591\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 3.918540\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 3.805367\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 3.974814\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 4.157548\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 3.886724\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 3.951857\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 3.718363\n",
      "\n",
      "Test set: Average loss: -4.0620, Accuracy: 8842/10000 (88%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 3.523736\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 3.742672\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 4.219471\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 3.682926\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 3.990790\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 3.933951\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 3.974385\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 3.773015\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 4.206933\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 4.251743\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 4.079597\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 3.819624\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 3.809232\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 3.652156\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 4.162459\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 4.009713\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 4.419089\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 4.019788\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 4.004090\n",
      "\n",
      "Test set: Average loss: -4.1793, Accuracy: 8922/10000 (89%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 3.833285\n",
      "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 4.185199\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 3.865971\n",
      "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 4.141088\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 4.115474\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 3.868086\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 3.888312\n",
      "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 4.303308\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 3.864576\n",
      "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 3.941428\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 4.022813\n",
      "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 3.651809\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 4.030233\n",
      "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 3.990216\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 4.120761\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 3.833328\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 3.969446\n",
      "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 3.857518\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 3.769335\n",
      "\n",
      "Test set: Average loss: -4.3056, Accuracy: 8977/10000 (90%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 3.722870\n",
      "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 3.562940\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 3.881095\n",
      "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 3.693639\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 3.968684\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 3.813396\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 4.018023\n",
      "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 3.915027\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 3.566080\n",
      "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 4.099984\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 3.935795\n",
      "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 3.927818\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 3.616024\n",
      "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 3.637884\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 3.772991\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 4.092587\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 4.262442\n",
      "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 3.670186\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 4.141557\n",
      "\n",
      "Test set: Average loss: -4.3536, Accuracy: 8995/10000 (90%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 4.021043\n",
      "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 3.860250\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 4.019301\n",
      "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 3.805002\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 4.063840\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 4.176250\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 3.873773\n",
      "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 3.998388\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 4.197758\n",
      "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 3.994111\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 3.818858\n",
      "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 4.190688\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 3.873191\n",
      "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 3.979726\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 3.871819\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 4.022751\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 4.224866\n",
      "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 4.081771\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 3.990014\n",
      "\n",
      "Test set: Average loss: -4.3978, Accuracy: 9013/10000 (90%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 4.078586\n",
      "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 3.725965\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 3.749224\n",
      "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 3.659453\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 3.897772\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 4.064147\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 3.689137\n",
      "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 3.918907\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 3.708076\n",
      "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 3.573282\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 3.846002\n",
      "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 3.897953\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 3.629851\n",
      "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 3.900723\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 4.326367\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 3.935972\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 3.850580\n",
      "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 3.785100\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 3.872960\n",
      "\n",
      "Test set: Average loss: -4.4012, Accuracy: 9027/10000 (90%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 4.022011\n",
      "Train Epoch: 10 [3200/60000 (5%)]\tLoss: 3.501580\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 4.260320\n",
      "Train Epoch: 10 [9600/60000 (16%)]\tLoss: 3.897254\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 4.004217\n",
      "Train Epoch: 10 [16000/60000 (27%)]\tLoss: 3.863884\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 3.909880\n",
      "Train Epoch: 10 [22400/60000 (37%)]\tLoss: 3.784362\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 4.299600\n",
      "Train Epoch: 10 [28800/60000 (48%)]\tLoss: 3.922550\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 3.824865\n",
      "Train Epoch: 10 [35200/60000 (59%)]\tLoss: 3.925618\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 3.670797\n",
      "Train Epoch: 10 [41600/60000 (69%)]\tLoss: 3.529396\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 3.670093\n",
      "Train Epoch: 10 [48000/60000 (80%)]\tLoss: 4.047216\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 3.736398\n",
      "Train Epoch: 10 [54400/60000 (91%)]\tLoss: 3.540726\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 3.694289\n",
      "\n",
      "Test set: Average loss: -4.4349, Accuracy: 9039/10000 (90%)\n",
      "\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 3.846589\n",
      "Train Epoch: 11 [3200/60000 (5%)]\tLoss: 3.717012\n",
      "Train Epoch: 11 [6400/60000 (11%)]\tLoss: 3.784266\n",
      "Train Epoch: 11 [9600/60000 (16%)]\tLoss: 4.209812\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 3.670539\n",
      "Train Epoch: 11 [16000/60000 (27%)]\tLoss: 3.461443\n",
      "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 3.744192\n",
      "Train Epoch: 11 [22400/60000 (37%)]\tLoss: 3.510619\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 4.163385\n",
      "Train Epoch: 11 [28800/60000 (48%)]\tLoss: 3.690825\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 3.718394\n",
      "Train Epoch: 11 [35200/60000 (59%)]\tLoss: 3.581786\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 3.595545\n",
      "Train Epoch: 11 [41600/60000 (69%)]\tLoss: 3.905444\n",
      "Train Epoch: 11 [44800/60000 (75%)]\tLoss: 3.531826\n",
      "Train Epoch: 11 [48000/60000 (80%)]\tLoss: 3.790978\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 4.061541\n",
      "Train Epoch: 11 [54400/60000 (91%)]\tLoss: 3.638324\n",
      "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 3.820314\n",
      "\n",
      "Test set: Average loss: -4.4566, Accuracy: 9045/10000 (90%)\n",
      "\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 3.781811\n",
      "Train Epoch: 12 [3200/60000 (5%)]\tLoss: 3.761289\n",
      "Train Epoch: 12 [6400/60000 (11%)]\tLoss: 4.052076\n",
      "Train Epoch: 12 [9600/60000 (16%)]\tLoss: 3.656486\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 4.102581\n",
      "Train Epoch: 12 [16000/60000 (27%)]\tLoss: 3.661437\n",
      "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 4.011083\n",
      "Train Epoch: 12 [22400/60000 (37%)]\tLoss: 4.334505\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 4.274971\n",
      "Train Epoch: 12 [28800/60000 (48%)]\tLoss: 4.493247\n",
      "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 3.709904\n",
      "Train Epoch: 12 [35200/60000 (59%)]\tLoss: 3.925976\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 3.898552\n",
      "Train Epoch: 12 [41600/60000 (69%)]\tLoss: 3.490985\n",
      "Train Epoch: 12 [44800/60000 (75%)]\tLoss: 3.645562\n",
      "Train Epoch: 12 [48000/60000 (80%)]\tLoss: 4.245837\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 3.402437\n",
      "Train Epoch: 12 [54400/60000 (91%)]\tLoss: 3.770808\n",
      "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 3.761111\n",
      "\n",
      "Test set: Average loss: -4.4676, Accuracy: 9050/10000 (90%)\n",
      "\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 3.765964\n",
      "Train Epoch: 13 [3200/60000 (5%)]\tLoss: 3.760806\n",
      "Train Epoch: 13 [6400/60000 (11%)]\tLoss: 3.962343\n",
      "Train Epoch: 13 [9600/60000 (16%)]\tLoss: 3.738992\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 3.823812\n",
      "Train Epoch: 13 [16000/60000 (27%)]\tLoss: 3.964275\n",
      "Train Epoch: 13 [19200/60000 (32%)]\tLoss: 3.811270\n",
      "Train Epoch: 13 [22400/60000 (37%)]\tLoss: 4.304977\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 4.068507\n",
      "Train Epoch: 13 [28800/60000 (48%)]\tLoss: 3.969099\n",
      "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 4.154483\n",
      "Train Epoch: 13 [35200/60000 (59%)]\tLoss: 4.085246\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 3.600132\n",
      "Train Epoch: 13 [41600/60000 (69%)]\tLoss: 3.935907\n",
      "Train Epoch: 13 [44800/60000 (75%)]\tLoss: 3.855429\n",
      "Train Epoch: 13 [48000/60000 (80%)]\tLoss: 3.968681\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 4.173666\n",
      "Train Epoch: 13 [54400/60000 (91%)]\tLoss: 4.175144\n",
      "Train Epoch: 13 [57600/60000 (96%)]\tLoss: 3.819764\n",
      "\n",
      "Test set: Average loss: -4.4697, Accuracy: 9052/10000 (91%)\n",
      "\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 3.544377\n",
      "Train Epoch: 14 [3200/60000 (5%)]\tLoss: 3.772470\n",
      "Train Epoch: 14 [6400/60000 (11%)]\tLoss: 3.697813\n",
      "Train Epoch: 14 [9600/60000 (16%)]\tLoss: 3.537581\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 3.712519\n",
      "Train Epoch: 14 [16000/60000 (27%)]\tLoss: 3.487117\n",
      "Train Epoch: 14 [19200/60000 (32%)]\tLoss: 3.544963\n",
      "Train Epoch: 14 [22400/60000 (37%)]\tLoss: 4.138585\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 3.912640\n",
      "Train Epoch: 14 [28800/60000 (48%)]\tLoss: 3.934377\n",
      "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 3.802471\n",
      "Train Epoch: 14 [35200/60000 (59%)]\tLoss: 3.973102\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 3.791053\n",
      "Train Epoch: 14 [41600/60000 (69%)]\tLoss: 3.671159\n",
      "Train Epoch: 14 [44800/60000 (75%)]\tLoss: 3.551264\n",
      "Train Epoch: 14 [48000/60000 (80%)]\tLoss: 3.893410\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 3.891656\n",
      "Train Epoch: 14 [54400/60000 (91%)]\tLoss: 4.001394\n",
      "Train Epoch: 14 [57600/60000 (96%)]\tLoss: 4.045744\n",
      "\n",
      "Test set: Average loss: -4.4713, Accuracy: 9053/10000 (91%)\n",
      "\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 4.105970\n",
      "Train Epoch: 15 [3200/60000 (5%)]\tLoss: 4.037856\n",
      "Train Epoch: 15 [6400/60000 (11%)]\tLoss: 3.885900\n",
      "Train Epoch: 15 [9600/60000 (16%)]\tLoss: 3.691479\n",
      "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 4.206307\n",
      "Train Epoch: 15 [16000/60000 (27%)]\tLoss: 3.790062\n",
      "Train Epoch: 15 [19200/60000 (32%)]\tLoss: 4.161022\n",
      "Train Epoch: 15 [22400/60000 (37%)]\tLoss: 3.958510\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 3.700043\n",
      "Train Epoch: 15 [28800/60000 (48%)]\tLoss: 3.738634\n",
      "Train Epoch: 15 [32000/60000 (53%)]\tLoss: 3.805036\n",
      "Train Epoch: 15 [35200/60000 (59%)]\tLoss: 3.788564\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 3.587350\n",
      "Train Epoch: 15 [41600/60000 (69%)]\tLoss: 3.755724\n",
      "Train Epoch: 15 [44800/60000 (75%)]\tLoss: 3.856058\n",
      "Train Epoch: 15 [48000/60000 (80%)]\tLoss: 3.800367\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 3.805146\n",
      "Train Epoch: 15 [54400/60000 (91%)]\tLoss: 4.137582\n",
      "Train Epoch: 15 [57600/60000 (96%)]\tLoss: 3.749892\n",
      "\n",
      "Test set: Average loss: -4.4756, Accuracy: 9056/10000 (91%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LHRAMOuOfx1W"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMmKxoS5aexhh+sX897D0T4",
   "collapsed_sections": [],
   "include_colab_link": true,
   "mount_file_id": "1phTjfMRFndPHlUwdNJlWapZz4hTWyDL9",
   "name": "HW3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
